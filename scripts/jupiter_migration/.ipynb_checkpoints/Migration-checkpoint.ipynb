{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MAIN CONTROLLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from config import sparqlTerms, mig_ns, sparql_mig_test, sparql_mig_simple, sparql_mig_dev, vocabs\n",
    "from SPARQLWrapper import JSON, SPARQLWrapper\n",
    "from utilities import removeNS, PrintException\n",
    "import re, os, concurrent.futures, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():    \n",
    "    #  Iterate over every type of object that needs to be migrated. \n",
    "    #  This is the first splitting of the data for migration.\n",
    "    for ptype in [\"collection\", \"community\", \"generic\", \"thesis\", \"batch\"]:\n",
    "        # a queryObject knows where it came from.\n",
    "        # a queryObject has been split into multiple groups\n",
    "        # only one group exists for community, and one for collection objects\n",
    "        # approximately a thousand queries each are minted for thesis and for generic objects\n",
    "        # these queries are based on the first two digits in the fedora pair tree\n",
    "        # the \"refreshGroups\" and \"refreshQueries\" variables determine whether the transformation will query \n",
    "        ## the triplestore to set groups and queries and update the cache, or whether just to access the cache\n",
    "        # a refresh should occur at the time of migration.\n",
    "        \n",
    "        queryObject = QueryFactory.getMigrationQuery(ptype, refreshGroups = True, refreshQueries = True)\n",
    "        print('%s batch queries generated' % (ptype))\n",
    "        print('%i batch(es) of %s objects to be transformed' % (len(queryObject.queries), ptype))\n",
    "        i = 0\n",
    "        for group in queryObject.queries.keys():\n",
    "            i = i + 1\n",
    "            DTO = DataFactory.getData(queryObject.queries[group], group, queryObject) # query, group, object\n",
    "            DTO.transformData()\n",
    "            print(\"%i of %i %s batches transformed\" % (i, len(queryObject.queries), ptype) )\n",
    "        print(\"%s objects transformation completed\" % (ptype) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TRANSFORMATIONS\n",
    "#### functions for handling data passed over by the data object. Takes a triple, detects what kind of action needs to be taken based on the predicate, sends it to the appropriate function for transformations, then returns it back to the data handler to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Transformation():\n",
    "    \n",
    "    \"\"\"\n",
    "    the output must be a list of triples matching the same format as the input (as follows):\n",
    "    \n",
    "    {\n",
    "        'subject': {\n",
    "            'value': 'http://gillingham.library.ualberta.ca:8080/fedora/rest/prod/0r/96/76/28/0r967628d', \n",
    "            'type': 'uri'\n",
    "        }, \n",
    "        'predicate': {\n",
    "            'value': 'http://purl.org/dc/elements/1.1/subject', \n",
    "            'type': 'uri'\n",
    "        }, \n",
    "        'object': {\n",
    "            'value': 'Geochemistry', \n",
    "            'type': 'literal'\n",
    "        }\n",
    "    }\n",
    "    output is appended to self.output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.output = []\n",
    "        \n",
    "    ############################################################################\n",
    "    ######################## transformation on rdf:type ########################\n",
    "    ############################################################################\n",
    "        \n",
    "    def rdfsyntaxnstype(self, triple, ptype):\n",
    "        self.output.append(triple)\n",
    "        return self.output\n",
    "\n",
    "       \n",
    "    ############################################################################\n",
    "    ######################## transformation on dcterms:language ################\n",
    "    ############################################################################\n",
    "\n",
    "    def language(self, triple, ptype):\n",
    "        # normalize values and convert to URI (consult the \"vocabs\" variable from the config file (this folder))\n",
    "        self.output.append(triple)\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    ######################## transformation on dc:rights #######################\n",
    "    ############################################################################\n",
    "    \n",
    "    \n",
    "    def rights(self, triple, ptype):\n",
    "        #### \n",
    "        # several different license values need to be coerced into one common value, this needs to be confirmed with leah before it is written\n",
    "        self.output.append(triple)\n",
    "        return self.output\n",
    "\n",
    "    \n",
    "    ############################################################################\n",
    "    ######################## transformation on ual:institution #################\n",
    "    ############################################################################\n",
    "\n",
    "    def institution(self, triple, ptype):\n",
    "        # convert university of alberta to <http://id.loc.gov/authorities/names/n79058482>\n",
    "        self.output.append(triple)\n",
    "        return self.output\n",
    " \n",
    "\n",
    "    ############################################################################\n",
    "    ######################## transformation on dcterms:license #################\n",
    "    ############################################################################\n",
    "\n",
    "    \n",
    "    def license(self, triple, ptype):\n",
    "        #### \n",
    "        # convert licenses from text to URI (use vocabs variable, some coersion will be necessary)\n",
    "        self.output.append(triple)\n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    ######################## transformation on dcterms:type ####################\n",
    "    ############################################################################\n",
    "    \n",
    "    def type(self, triple, ptype):      \n",
    "        if ptype == 'batch':\n",
    "                # null\n",
    "                # Complete\n",
    "                # processing\n",
    "            self.output.append(\n",
    "                 {\n",
    "                    'subject': {\n",
    "                        'value': triple['subject']['value'], # the subject of the triple\n",
    "                        'type': 'uri'\n",
    "                    }, \n",
    "                    'predicate': {\n",
    "                        'value': \"a different predicate\", # the predicate of the triple\n",
    "                        'type': 'uri'\n",
    "                    }, \n",
    "                    'object': {\n",
    "                        'value': triple['object']['value'], # mapped uri\n",
    "                        'type': 'uri'\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        if ptype == 'thesis':\n",
    "            # nothing needs to happen\n",
    "            pass\n",
    "        elif ptype == 'generic':\n",
    "            for vocab in vocabs[\"type\"]:\n",
    "                # mint a new triple with the mapped type\n",
    "                if triple['object']['value'] in vocab[\"mapping\"]:\n",
    "                    self.output.append(\n",
    "                        {\n",
    "                            'subject': {\n",
    "                                'value': triple['subject']['value'], # the subject of the triple\n",
    "                                'type': 'uri'\n",
    "                            }, \n",
    "                            'predicate': {\n",
    "                                'value': triple['predicate']['value'], # the predicate of the triple\n",
    "                                'type': 'uri'\n",
    "                            }, \n",
    "                            'object': {\n",
    "                                'value': vocab[\"uri\"], # mapped uri\n",
    "                                'type': 'uri'\n",
    "                            }\n",
    "                        }\n",
    "                    )\n",
    "                if \"Draft-Submitted\" in triple['object']['value']:\n",
    "                    self.output.append( \n",
    "                        {\n",
    "                            'subject': {\n",
    "                                'value': triple['subject']['value'], # the subject of the triple\n",
    "                                'type': 'uri'\n",
    "                            }, \n",
    "                            'predicate': {\n",
    "                                'value': \"http://purl.org/ontology/bibo/status\", # the predicate of the triple\n",
    "                                'type': 'uri'\n",
    "                            }, \n",
    "                            'object': {\n",
    "                                'value': \"http://vivoweb.org/ontology/core#submitted\", # mapped uri\n",
    "                                'type': 'uri'\n",
    "                            }\n",
    "                        }\n",
    "                    )\n",
    "                    self.output.append(\n",
    "                         {\n",
    "                            'subject': {\n",
    "                                'value': triple['subject']['value'], # the subject of the triple\n",
    "                                'type': 'uri'\n",
    "                            }, \n",
    "                            'predicate': {\n",
    "                                'value': \"http://purl.org/ontology/bibo/status\", # the predicate of the triple\n",
    "                                'type': 'uri'\n",
    "                            }, \n",
    "                            'object': {\n",
    "                                'value': \"http://purl.org/ontology/bibo/status#draft\", # mapped uri\n",
    "                                'type': 'uri'\n",
    "                            }\n",
    "                        }\n",
    "                    )\n",
    "                if \"Published\" in triple['object']['value']:\n",
    "                    self.output.append( \n",
    "                         {\n",
    "                            'subject': {\n",
    "                                'value': triple['subject']['value'], # the subject of the triple\n",
    "                                'type': 'uri'\n",
    "                            }, \n",
    "                            'predicate': {\n",
    "                                'value': \"http://purl.org/ontology/bibo/status\", # the predicate of the triple\n",
    "                                'type': 'uri'\n",
    "                            }, \n",
    "                            'object': {\n",
    "                                'value': \"http://purl.org/ontology/bibo/status#published\", # mapped uri\n",
    "                                'type': 'uri'\n",
    "                            }\n",
    "                        }\n",
    "                    )\n",
    "        elif (ptype == 'community') or (ptype == 'collection'):\n",
    "            self.output.append(triple)\n",
    "        \n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  QUERY BUILDER\n",
    "##### Pulls current mappings from triplestore, dynamically builds queries in managable sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Query(object):\n",
    "    \"\"\" Query objects are dynamically generated, and contain SPARQL CONSTRUCT queries with input from the jupiter application profile \"\"\"\n",
    "    def __init__(self, ptype, sparqlTerms=sparqlTerms, sparqlData=sparql_mig_test):\n",
    "        self.mapping = []\n",
    "        self.sparqlTerms = SPARQLWrapper(sparqlTerms)  # doesn't need to change (the terms store doesn't change)\n",
    "        self.sparqlData = SPARQLWrapper(sparqlData)  # sets the triple store from which to get data (simple, test, or dev)\n",
    "        self.endpoint = sparqlData\n",
    "        self.queries = {}\n",
    "        self.splitBy = {}\n",
    "        self.prefixes = \"\"\n",
    "        self.filename = \"\"\n",
    "\n",
    "        for ns in mig_ns:\n",
    "            self.prefixes = self.prefixes + \" PREFIX %s: <%s> \" % (ns['prefix'], ns['uri'])\n",
    "        self.getMappings()\n",
    "        self.generateQueries()\n",
    "\n",
    "    def getMappings(self):\n",
    "        query = \"prefix ual: <http://terms.library.ualberta.ca/>SELECT * WHERE {GRAPH ual:%s {?newProperty ual:backwardCompatibleWith ?oldProperty} }\" % (self.ptype)\n",
    "        self.sparqlTerms.setReturnFormat(JSON)\n",
    "        self.sparqlTerms.setQuery(query)\n",
    "        results = self.sparqlTerms.query().convert()\n",
    "        for result in results['results']['bindings']:\n",
    "            self.mapping.append((result['newProperty']['value'], result['oldProperty']['value']))\n",
    "\n",
    "    def generateQueries(self):\n",
    "        pass\n",
    "    \n",
    "    def getSplitBy(self):\n",
    "        # checks if we are taking from the cache, or if we are going to refresh the cache\n",
    "        if self.refreshGroups is True:\n",
    "            # base query only needs 3 prefixes appended to the \"select\" statement defined by the object\n",
    "            query = \"prefix dcterm: <http://purl.org/dc/terms/> prefix info: <info:fedora/fedora-system:def/model#> prefix ual: <http://terms.library.ualberta.ca/> %s\" % (self.select)\n",
    "            self.sparqlData.setReturnFormat(JSON)\n",
    "            self.sparqlData.setQuery(query)\n",
    "            results =  self.sparqlData.query().convert()\n",
    "            # iterate over query results\n",
    "            for result in results['results']['bindings']:\n",
    "                # the group is the two folders at the base of the pair tree, concatenated by an underscore\n",
    "                group = result['resource']['value'].split('/')[6]\n",
    "                # assign that parameter by which you want to search to that group\n",
    "                self.splitBy[group] = \"/\".join( result['resource']['value'].split('/')[:7] )# the stem of the resource [0] and the group number by which to save [1] (this is the first digit in the pair tree)\n",
    "            filename = \"cache/groups/%s.txt\" % (self.ptype)\n",
    "            # write the group parameters to this file\n",
    "            with open(filename, 'w+') as f:\n",
    "                json.dump([self.splitBy], f)                \n",
    "        # if we are reading from the cache\n",
    "        elif self.refresh is False:\n",
    "            # iterate over the group files, for each file, read the resources into a group list\n",
    "            filename = \"cache/groups/%s.json\" % (self.ptype)\n",
    "            with open(filename, 'r+') as f:\n",
    "                self.splitBy = json.load(f)\n",
    "                # self.splitBy['_'.join(line.split('/')[2:])] = line\n",
    "\n",
    "    def writeQueries(self):\n",
    "        filename = \"cache/queries/%s.json\" % (self.ptype)\n",
    "        with open(filename, 'w+') as f:\n",
    "            json.dump([self.queries], f)\n",
    "        \n",
    "    def readQueries(self):\n",
    "        filename = \"cache/queries/%s.json\" % (self.ptype)\n",
    "        with open(filename, 'r+') as f:\n",
    "            self.queries = json.load(f)\n",
    "                \n",
    "class Collection(Query):\n",
    "    def __init__(self, refreshQueries):\n",
    "        self.refreshQueries = refreshQueries\n",
    "        self.ptype = 'collection'\n",
    "        self.construct = \"CONSTRUCT { ?resource info:hasModel 'IRItem'^^xsd:string ; rdf:type pcdm:Collection\"\n",
    "        self.where = [\"WHERE { ?resource info:hasModel 'Collection'^^xsd:string . OPTIONAL { ?resource ualids:is_community 'false'^^xsd:boolean } . OPTIONAL { ?resource ualid:is_community 'false'^^xsd:boolean } . OPTIONAL { ?resource ual:is_community 'false'^^xsd:boolean }\"]\n",
    "        self.select = None\n",
    "        super().__init__(self.ptype)\n",
    "\n",
    "    def generateQueries(self):\n",
    "        if self.refreshQueries is True:\n",
    "            for where in self.where:\n",
    "                construct = self.construct\n",
    "                for pair in self.mapping:\n",
    "                    construct = \"%s ; <%s> ?%s\" % (construct, pair[0], removeNS(pair[0]))\n",
    "                    where = \" %s . OPTIONAL { ?resource <%s> ?%s . FILTER (?%s!='') }\" % (where, pair[1], removeNS(pair[0]), removeNS(pair[0]))\n",
    "                self.queries['collection'] = \"%s %s } %s }\" % (self.prefixes, construct, where)\n",
    "            self.writeQueries\n",
    "        elif self.refreshQueries is False:\n",
    "            self.readQueries()\n",
    "\n",
    "class Community(Query):\n",
    "    def __init__(self, refreshQueries):\n",
    "        self.refreshQueries = refreshQueries\n",
    "        self.ptype = 'community'\n",
    "        self.construct = \"CONSTRUCT { ?resource info:hasModel 'IRItem'^^xsd:string ; rdf:type pcdm:Object; rdf:type ual:Community\"\n",
    "        self.where = [\"WHERE { ?resource info:hasModel 'Collection'^^xsd:string ; OPTIONAL { ?resource ualids:is_community 'true'^^xsd:boolean } . OPTIONAL { ?resource ualid:is_community 'true'^^xsd:boolean } . OPTIONAL { ?resource ual:is_community 'true'^^xsd:boolean }\"]\n",
    "        self.select = None\n",
    "        super().__init__(self.ptype)\n",
    "\n",
    "    def generateQueries(self):\n",
    "        if self.refreshQueries is True:\n",
    "            for where in self.where:\n",
    "                construct = self.construct\n",
    "                for pair in self.mapping:\n",
    "                    construct = \"%s ; <%s> ?%s\" % (construct, pair[0], removeNS(pair[0]))\n",
    "                    where = \" %s . OPTIONAL { ?resource <%s> ?%s . FILTER (?%s!='') }\" % (where, pair[1], removeNS(pair[0]), removeNS(pair[0]))\n",
    "                self.queries['community'] = \"%s %s } %s }\" % (self.prefixes, construct, where)\n",
    "            self.writeQueries()\n",
    "        elif self.refreshQueries is False:\n",
    "            self.readQueries()\n",
    "\n",
    "class Generic(Query):\n",
    "    def __init__(self, refreshGroups, refreshQueries):\n",
    "        self.refreshGroups = refreshGroups\n",
    "        self.refreshQueries = refreshQueries\n",
    "        self.ptype = 'generic'\n",
    "        self.construct = \"CONSTRUCT { ?resource info:hasModel 'IRItem'^^xsd:string ; rdf:type pcdm:Object; rdf:type works:work\"\n",
    "        self.where = []\n",
    "        self.select = \"SELECT distinct ?resource WHERE { ?resource info:hasModel 'GenericFile'^^xsd:string ; dcterm:type ?type . filter(?type != 'Thesis'^^xsd:string) }\"\n",
    "        super().__init__(self.ptype)\n",
    "\n",
    "    def generateQueries(self):\n",
    "        self.getSplitBy()\n",
    "        if self.refreshQueries is True:\n",
    "            query = \"%s %s\" % (self.prefixes, self.select)\n",
    "            for group in self.splitBy.keys():\n",
    "                where = \"WHERE {  ?resource info:hasModel 'GenericFile'^^xsd:string ; dcterm:type ?type . filter(?type != 'Thesis'^^xsd:string) . FILTER (contains(str(?resource), '%s'))\" % (self.splitBy[group])\n",
    "                construct = self.construct\n",
    "                for pair in self.mapping:\n",
    "                    construct = \"%s ; <%s> ?%s\" % (construct, pair[0], removeNS(pair[0]))\n",
    "                    where = \" %s . OPTIONAL { ?resource <%s> ?%s . FILTER (?%s!='') }\" % (where, pair[1], removeNS(pair[0]), removeNS(pair[0]))\n",
    "                self.queries[group] = \"%s %s } %s }\" % (self.prefixes, construct, where)\n",
    "            self.writeQueries()\n",
    "        elif self.refreshQueries is False:\n",
    "            self.readQueries()\n",
    "\n",
    "class Thesis(Query):\n",
    "    def __init__(self, refreshGroups, refreshQueries):\n",
    "        self.refreshGroups = refreshGroups\n",
    "        self.refreshQueries = refreshQueries\n",
    "        self.ptype = 'thesis'\n",
    "        self.construct = \"CONSTRUCT { ?resource info:hasModel 'IRItem'^^xsd:string ; rdf:type pcdm:Object; rdf:type works:work ; rdf:type bibo:Thesis\"\n",
    "        self.where = []\n",
    "        self.select = \"SELECT distinct ?resource WHERE { ?resource info:hasModel 'GenericFile'^^xsd:string ; dcterm:type 'Thesis'^^xsd:string }\"\n",
    "        super().__init__(self.ptype)\n",
    "\n",
    "    def generateQueries(self):\n",
    "        self.getSplitBy()        \n",
    "        if self.refreshQueries is True:\n",
    "            query = \"%s %s\" % (self.prefixes, self.select)\n",
    "            for group in self.splitBy.keys():\n",
    "                where = \"WHERE { ?resource info:hasModel 'GenericFile'^^xsd:string ; dcterm:type 'Thesis'^^xsd:string . FILTER (contains(str(?resource), '%s'))\" % (self.splitBy[group])\n",
    "                construct = self.construct\n",
    "                for pair in self.mapping:\n",
    "                    construct = \"%s ; <%s> ?%s\" % (construct, pair[0], removeNS(pair[0]))\n",
    "                    where = \" %s . OPTIONAL { ?resource <%s> ?%s . FILTER (?%s!='') }\" % (where, pair[1], removeNS(pair[0]), removeNS(pair[0]))\n",
    "                    self.queries[group] =  \"%s %s } %s  }\" % (self.prefixes, construct, where)\n",
    "            self.writeQueries()\n",
    "        elif self.refreshQueries is False:\n",
    "            self.readQueries()\n",
    "\n",
    "\n",
    "class Batch(Query):\n",
    "    def __init__(self, refreshGroups, refreshQueries):\n",
    "        self.refreshGroups = refreshGroups\n",
    "        self.refreshQueries = refreshQueries\n",
    "        self.ptype = 'batch'\n",
    "        self.construct = \"CONSTRUCT { ?resource info:hasModel 'Batch' ; schema:result ?type; ?predicate ?object }\"\n",
    "        self.where = \"WHERE { ?resource info:hasModel 'Batch'^^xsd:string ; \"\n",
    "        self.select = \"SELECT distinct ?resource WHERE  { ?resource info:hasModel 'Batch'^^xsd:string } \"\n",
    "        super().__init__(self.ptype)\n",
    "    \n",
    "    def generateQueries(self):\n",
    "        self.getSplitBy()       \n",
    "        if self.refreshQueries is True:\n",
    "            for group in self.splitBy.keys():\n",
    "                self.queries[group] = \"%s %s %s FILTER (contains(str(?resource), '%s')) . ?resource dcterm:type ?type . ?resource ?predicate ?object . FILTER (?object!='') }\" % (self.prefixes, self.construct, self.where, self.splitBy[group])\n",
    "            self.writeQueries()\n",
    "        elif self.refreshQueries is False:\n",
    "            self.readQueries()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DATA TRANSPORT OBJECTS\n",
    "##### Runs a query, sends data to get transformed, saves data to appropriate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    def __init__(self, query, group, sparqlData, sparqlTerms, ptype):\n",
    "        self.q = query\n",
    "        self.group = group\n",
    "        self.sparqlData = sparqlData\n",
    "        self.sparqlTerms = sparqlTerms\n",
    "        self.filename = \"\"\n",
    "        self.output = []\n",
    "        self.ptype = ptype\n",
    "\n",
    "    def transformData(self):\n",
    "        self.sparqlData.setMethod(\"GET\")\n",
    "        self.sparqlData.setReturnFormat(JSON)\n",
    "        self.sparqlData.setQuery(self.q)\n",
    "        results = self.sparqlData.query().convert()['results']['bindings']\n",
    "        for result in results:\n",
    "            result = TransformationFactory().getTransformation(result, self.ptype)\n",
    "            if isinstance(result, list):\n",
    "                for triple in result:\n",
    "                    s = \"<%s>\" % (str(triple['subject']['value']))\n",
    "                    p = \"<%s>\" % (str(triple['predicate']['value']))\n",
    "                    if triple['object']['type'] == 'uri':\n",
    "                        o = \"<%s>\" % (str(triple['object']['value']))\n",
    "                    else:\n",
    "                        o = \"\\\"%s\\\"\" % (str(triple['object']['value']))\n",
    "                    self.output.append(\"%s %s %s . \\n\" % (s, p, o))\n",
    "        with open(self.filename, \"w+\") as f:\n",
    "            f.writelines(self.output)\n",
    "\n",
    "        #with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        #    future_to_result = {executor.submit(self.processResults, results, query): result for result in results}\n",
    "        #    for future in concurrent.futures.as_completed(future_to_result):\n",
    "        #        result = future_to_result[future]\n",
    "        #        try:\n",
    "        #            future.result()\n",
    "        #        except Exception:\n",
    "        #            PrintException()\n",
    "\n",
    "class CollectionData(Data):\n",
    "    def __init__(self, q, group, sparqlData, sparqlTerms, ptype):\n",
    "        super().__init__(q, group, sparqlData, sparqlTerms, ptype)\n",
    "        self.filename = \"results/collection.nt\"\n",
    "\n",
    "class CommunityData(Data):\n",
    "    def __init__(self, q, group, sparqlData, sparqlTerms, ptype):\n",
    "        super().__init__(q, group, sparqlData, sparqlTerms, ptype)\n",
    "        self.filename = 'results/community.nt'\n",
    "\n",
    "\n",
    "class ThesisData(Data):\n",
    "    def __init__(self, q, group, sparqlData, sparqlTerms, ptype):\n",
    "        super().__init__(q, group, sparqlData, sparqlTerms, ptype)\n",
    "        self.filename = \"results/%s/%s.nt\" % (self.ptype, self.group)\n",
    "\n",
    "\n",
    "class GenericData(Data):\n",
    "    def __init__(self, q, group, sparqlData, sparqlTerms, ptype):\n",
    "        super().__init__(q, group, sparqlData, sparqlTerms, ptype)\n",
    "        self.filename = \"results/%s/%s.nt\" % (self.ptype, self.group)\n",
    "\n",
    "class BatchData(Data):\n",
    "    def __init__(self, q, group, sparqlData, sparqlTerms, ptype):\n",
    "        super().__init__(q, group, sparqlData, sparqlTerms, ptype)\n",
    "        self.filename = \"results/%s/%s.nt\" % (self.ptype, self.group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QueryFactory():\n",
    "    @staticmethod\n",
    "    def getMigrationQuery(ptype, refreshGroups, refreshQueries):\n",
    "        \"\"\" returns a specified query object depending on the type passed in\"\"\"\n",
    "        functions = {\"collection\": Collection(refreshQueries), \n",
    "                     \"community\": Community(refreshQueries), \n",
    "                     \"thesis\": Thesis(refreshGroups, refreshQueries), \n",
    "                     \"generic\": Generic(refreshGroups, refreshQueries), \n",
    "                     \"batch\": Batch(refreshGroups, refreshQueries)\n",
    "                    }\n",
    "        if ptype in functions:\n",
    "            return functions[ptype]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFactory():\n",
    "    @staticmethod\n",
    "    def getData(query, group, queryObject):\n",
    "        \"\"\" returns a specified query object depending on the type passed in\"\"\"\n",
    "        functions = {\"collection\": CollectionData(query, group, queryObject.sparqlData, queryObject.sparqlTerms, queryObject.ptype), \n",
    "                     \"community\": CommunityData(query, group, queryObject.sparqlData, queryObject.sparqlTerms, queryObject.ptype), \n",
    "                     \"thesis\": ThesisData(query, group, queryObject.sparqlData, queryObject.sparqlTerms, queryObject.ptype), \n",
    "                     \"generic\": GenericData(query, group, queryObject.sparqlData, queryObject.sparqlTerms, queryObject.ptype), \n",
    "                     \"batch\": BatchData(query, group, queryObject.sparqlData, queryObject.sparqlTerms, queryObject.ptype)\n",
    "                    }\n",
    "        if queryObject.ptype in functions:\n",
    "            return functions[queryObject.ptype]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransformationFactory():\n",
    "    @staticmethod\n",
    "    def getTransformation(triple, ptype):\n",
    "        function = re.sub(r'[0-9]+', '', triple['predicate']['value'].split('/')[-1].replace('#', '').replace('-', ''))\n",
    "        functions = {\"rdfsyntaxnstype\": Transformation().rdfsyntaxnstype(triple, ptype),\n",
    "                     \"language\": Transformation().language(triple, ptype),\n",
    "                     \"type\": Transformation().type(triple, ptype),\n",
    "                     \"rights\": Transformation().rights(triple, ptype),\n",
    "                     \"license\": Transformation().license(triple, ptype)\n",
    "                    }\n",
    "        if function in functions:\n",
    "            return functions[function]\n",
    "        else:\n",
    "            return [triple]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collection batch queries generated\n",
      "1 batch(es) of collection objects to be transformed\n",
      "1 of 1 collection batches transformed\n",
      "collection objects transformation completed\n",
      "community batch queries generated\n",
      "1 batch(es) of community objects to be transformed\n",
      "1 of 1 community batches transformed\n",
      "community objects transformation completed\n",
      "generic batch queries generated\n",
      "1 batch(es) of generic objects to be transformed\n",
      "1 of 1 generic batches transformed\n",
      "generic objects transformation completed\n",
      "thesis batch queries generated\n",
      "0 batch(es) of thesis objects to be transformed\n",
      "thesis objects transformation completed\n",
      "batch batch queries generated\n",
      "1 batch(es) of batch objects to be transformed\n",
      "1 of 1 batch batches transformed\n",
      "batch objects transformation completed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
