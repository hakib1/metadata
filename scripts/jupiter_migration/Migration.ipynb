{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MAIN CONTROLLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import sparqlTerms, mig_ns, sparql_mig_test, sparql_mig_simple, sparql_mig_dev, vocabs, types\n",
    "from SPARQLWrapper import JSON, SPARQLWrapper\n",
    "from utilities import removeNS, PrintException, cleanOutputs\n",
    "import re, os, concurrent.futures, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    #  Iterate over every type of object that needs to be migrated. \n",
    "    #  This is the first splitting of the data for migration.\n",
    "    cleanOutputs(types)\n",
    "    for ptype in types:\n",
    "        # a queryObject knows where it came from.\n",
    "        # a queryObject has been split into multiple groups\n",
    "        # only one group exists for community, and one for collection objects\n",
    "        # approximately a thousand queries each are minted for thesis and for generic objects\n",
    "        # these queries are based on the first folder in the fedora pair tree\n",
    "   \n",
    "        queryObject = QueryFactory.getMigrationQuery(ptype, sparqlData=sparql_mig_test)\n",
    "        print('%s batch queries generated' % (ptype))\n",
    "        print('%i batch(es) of %s objects to be transformed' % (len(queryObject.queries), ptype))\n",
    "        i = 0\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "            \n",
    "            future_to_result = {executor.submit(parellelTransform, queryObject, group): group for group in queryObject.queries.keys()}\n",
    "            for future in concurrent.futures.as_completed(future_to_result):\n",
    "                result = future_to_result[future]\n",
    "                try:\n",
    "                    i = i + 1\n",
    "                    future.result()\n",
    "                    print(\"%i of %i %s batches transformed\" % (i, len(queryObject.queries), ptype) )\n",
    "                except Exception:\n",
    "                    PrintException()\n",
    "        print(\"%s objects transformation completed\" % (ptype) )\n",
    "\n",
    "def parellelTransform(queryObject, group):\n",
    "    DTO = DataFactory.getData(queryObject.queries[group], group, queryObject) # query, group, object\n",
    "    DTO.transformData()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TRANSFORMATIONS\n",
    "#### functions for handling data passed over by the data object. Takes a triple, detects what kind of action needs to be taken based on the predicate, sends it to the appropriate function for transformations, then returns it back to the data handler to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformation():\n",
    "    \n",
    "    \"\"\"\n",
    "    the output must be a list of triples matching the same format as the input (as follows):\n",
    "    \n",
    "    {\n",
    "        'subject': {\n",
    "            'value': 'http://gillingham.library.ualberta.ca:8080/fedora/rest/prod/0r/96/76/28/0r967628d', \n",
    "            'type': 'uri'\n",
    "        }, \n",
    "        'predicate': {\n",
    "            'value': 'http://purl.org/dc/elements/1.1/subject', \n",
    "            'type': 'uri'\n",
    "        }, \n",
    "        'object': {\n",
    "            'value': 'Geochemistry', \n",
    "            'type': 'literal'\n",
    "        }\n",
    "    }\n",
    "    output is appended to self.output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.output = []\n",
    "        \n",
    "    ############################################################################\n",
    "    ######################## transformation on rdf:type ########################\n",
    "    ############################################################################\n",
    "        \n",
    "    def rdfsyntaxnstype(self, triple, ptype):\n",
    "        self.output.append(triple)\n",
    "        return self.output\n",
    "\n",
    "       \n",
    "    ############################################################################\n",
    "    ######################## transformation on dcterms:language ################\n",
    "    ############################################################################\n",
    "\n",
    "    def language(self, triple, ptype):\n",
    "        # normalize values and convert to URI (consult the \"vocabs\" variable from the config file (this folder))\n",
    "        for vocab in vocabs[\"language\"]:\n",
    "            # mint a new triple with the mapped type\n",
    "            if triple['object']['value'] in  vocab[\"mapping\"]:\n",
    "                self.output.append(\n",
    "                    {\n",
    "                            'subject': {\n",
    "                                'value': triple['subject']['value'], # the subject of the triple\n",
    "                                'type': 'uri'\n",
    "                            }, \n",
    "                            'predicate': {\n",
    "                                'value': triple['predicate']['value'], # the predicate of the triple\n",
    "                                'type': 'uri'\n",
    "                            }, \n",
    "                            'object': {\n",
    "                                'value': vocab[\"uri\"], # mapped uri\n",
    "                                'type': 'uri'\n",
    "                            }\n",
    "                        } \n",
    "                    ) \n",
    "        self.output.append(triple)\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    ######################## transformation on dc:rights #######################\n",
    "    ############################################################################\n",
    "    \n",
    "    \n",
    "    def rights(self, triple, ptype):\n",
    "        #### \n",
    "        # several different license values need to be coerced into one common value, this needs to be confirmed with leah before it is written\n",
    "        self.output.append(triple)\n",
    "        return self.output\n",
    "\n",
    "    \n",
    "    ############################################################################\n",
    "    ######################## transformation on ual:institution #################\n",
    "    ############################################################################\n",
    "\n",
    "    def institution(self, triple, ptype):\n",
    "        # convert university of alberta to <http://id.loc.gov/authorities/names/n79058482>\n",
    "        self.output.append(triple)\n",
    "        return self.output\n",
    " \n",
    "\n",
    "    ############################################################################\n",
    "    ######################## transformation on dcterms:license #################\n",
    "    ############################################################################\n",
    "\n",
    "    \n",
    "    def license(self, triple, ptype):\n",
    "        #### \n",
    "        # convert licenses from text to URI (use vocabs variable, some coersion will be necessary)\n",
    "        self.output.append(triple)\n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    ######################## transformation on dcterms:type ####################\n",
    "    ############################################################################\n",
    "    \n",
    "    def type(self, triple, ptype):\n",
    "        if ptype == 'generic':\n",
    "            for vocab in vocabs[\"type\"]:\n",
    "                # mint a new triple with the mapped type\n",
    "                if triple['object']['value'] in vocab[\"mapping\"]:\n",
    "                    self.output.append(\n",
    "                        {\n",
    "                            'subject': {\n",
    "                                'value': triple['subject']['value'], # the subject of the triple\n",
    "                                'type': 'uri'\n",
    "                            }, \n",
    "                            'predicate': {\n",
    "                                'value': triple['predicate']['value'], # the predicate of the triple\n",
    "                                'type': 'uri'\n",
    "                            }, \n",
    "                            'object': {\n",
    "                                'value': vocab[\"uri\"], # mapped uri\n",
    "                                'type': 'uri'\n",
    "                            }\n",
    "                        }\n",
    "                    )\n",
    "     \n",
    "            else:\n",
    "                pass\n",
    "        elif (ptype == 'community') or (ptype == 'collection'):\n",
    "            self.output.append(triple)\n",
    "        \n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  QUERY BUILDER\n",
    "##### Pulls current mappings from triplestore, dynamically builds queries in managable sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query(object):\n",
    "    \"\"\" Query objects are dynamically generated, and contain SPARQL CONSTRUCT queries with input from the jupiter application profile \"\"\"\n",
    "    def __init__(self, ptype, sparqlData, sparqlTerms=sparqlTerms):\n",
    "        self.mapping = []\n",
    "        self.sparqlTerms = SPARQLWrapper(sparqlTerms)  # doesn't need to change (the terms store doesn't change)\n",
    "        self.sparqlData = SPARQLWrapper(sparqlData)  # sets the triple store from which to get data (simple, test, or dev)\n",
    "        self.endpoint = sparqlData\n",
    "        self.queries = {}\n",
    "        self.splitBy = {}\n",
    "        self.prefixes = \"\"\n",
    "        self.filename = \"\"\n",
    "        for ns in mig_ns:\n",
    "            self.prefixes = self.prefixes + \" PREFIX %s: <%s> \" % (ns['prefix'], ns['uri'])\n",
    "        self.getMappings()\n",
    "        self.generateQueries()\n",
    "\n",
    "    def getMappings(self):\n",
    "        query = \"prefix ual: <http://terms.library.ualberta.ca/>SELECT * WHERE {GRAPH ual:%s {?newProperty ual:backwardCompatibleWith ?oldProperty} }\" % (self.ptype)\n",
    "        self.sparqlTerms.setReturnFormat(JSON)\n",
    "        self.sparqlTerms.setQuery(query)\n",
    "        results = self.sparqlTerms.query().convert()\n",
    "        for result in results['results']['bindings']:\n",
    "            self.mapping.append((result['newProperty']['value'], result['oldProperty']['value']))\n",
    "\n",
    "    def getSplitBy(self):\n",
    "        # base query only needs 3 prefixes appended to the \"select\" statement defined by the object\n",
    "        query = \"prefix dcterm: <http://purl.org/dc/terms/> prefix info: <info:fedora/fedora-system:def/model#> prefix ual: <http://terms.library.ualberta.ca/> %s\" % (self.select)\n",
    "        self.sparqlData.setReturnFormat(JSON)\n",
    "        self.sparqlData.setQuery(query)\n",
    "        results =  self.sparqlData.query().convert()\n",
    "        # iterate over query results\n",
    "        for result in results['results']['bindings']:\n",
    "            # the group is the two folders at the base of the pair tree, concatenated by an underscore\n",
    "            group = result['resource']['value'].split('/')[6]\n",
    "            # assign that parameter by which you want to search to that group\n",
    "            self.splitBy[group] = \"/\".join( result['resource']['value'].split('/')[:7] )# the stem of the resource [0] and the group number by which to save [1] (this is the first digit in the pair tree)\n",
    "            \n",
    "\n",
    "    def generateQueries(self):\n",
    "        pass\n",
    "    \n",
    "    def writeQueries(self):\n",
    "        filename = \"cache/%s.json\" % (self.ptype)\n",
    "        with open(filename, 'w+') as f:\n",
    "            json.dump([self.queries], f)\n",
    "               \n",
    "class Collection(Query):\n",
    "    def __init__(self, sparqlData):\n",
    "        self.ptype = 'collection'\n",
    "        self.construct = \"CONSTRUCT { ?resource info:hasModel 'IRItem'^^xsd:string ; rdf:type pcdm:Collection\"\n",
    "        self.where = [\"WHERE { ?resource info:hasModel 'Collection'^^xsd:string . OPTIONAL { ?resource ualids:is_community 'false'^^xsd:boolean } . OPTIONAL { ?resource ualid:is_community 'false'^^xsd:boolean } . OPTIONAL { ?resource ual:is_community 'false'^^xsd:boolean }\"]\n",
    "        self.select = None\n",
    "        super().__init__(self.ptype, sparqlData)\n",
    "\n",
    "    def generateQueries(self):\n",
    "        for where in self.where:\n",
    "            construct = self.construct\n",
    "            for pair in self.mapping:\n",
    "                construct = \"%s ; <%s> ?%s\" % (construct, pair[0], removeNS(pair[0]))\n",
    "                where = \" %s . OPTIONAL { ?resource <%s> ?%s . FILTER (?%s!='') }\" % (where, pair[1], removeNS(pair[0]), removeNS(pair[0]))\n",
    "            self.queries['collection'] = \"%s %s } %s }\" % (self.prefixes, construct, where)\n",
    "        self.writeQueries\n",
    "\n",
    "class Community(Query):\n",
    "    def __init__(self, sparqlData):\n",
    "        self.ptype = 'community'\n",
    "        self.construct = \"CONSTRUCT { ?resource info:hasModel 'IRItem'^^xsd:string ; rdf:type pcdm:Object; rdf:type ual:Community\"\n",
    "        self.where = [\"WHERE { ?resource info:hasModel 'Collection'^^xsd:string ; OPTIONAL { ?resource ualids:is_community 'true'^^xsd:boolean } . OPTIONAL { ?resource ualid:is_community 'true'^^xsd:boolean } . OPTIONAL { ?resource ual:is_community 'true'^^xsd:boolean }\"]\n",
    "        self.select = None\n",
    "        super().__init__(self.ptype, sparqlData)\n",
    "\n",
    "    def generateQueries(self):\n",
    "        for where in self.where:\n",
    "            construct = self.construct\n",
    "            for pair in self.mapping:\n",
    "                construct = \"%s ; <%s> ?%s\" % (construct, pair[0], removeNS(pair[0]))\n",
    "                where = \" %s . OPTIONAL { ?resource <%s> ?%s . FILTER (?%s!='') }\" % (where, pair[1], removeNS(pair[0]), removeNS(pair[0]))\n",
    "            self.queries['community'] = \"%s %s } %s }\" % (self.prefixes, construct, where)\n",
    "        self.writeQueries()\n",
    "\n",
    "class Generic(Query):\n",
    "    def __init__(self, sparqlData):\n",
    "        self.ptype = 'generic'\n",
    "        self.construct = \"CONSTRUCT { ?resource info:hasModel 'IRItem'^^xsd:string ; rdf:type pcdm:Object; rdf:type works:work\"\n",
    "        self.where = []\n",
    "        self.select = \"SELECT distinct ?resource WHERE { ?resource info:hasModel 'GenericFile'^^xsd:string ; dcterm:type ?type . filter(?type != 'Thesis'^^xsd:string) }\"\n",
    "        super().__init__(self.ptype, sparqlData)\n",
    "\n",
    "    def generateQueries(self):\n",
    "        self.getSplitBy()\n",
    "        query = \"%s %s\" % (self.prefixes, self.select)\n",
    "        for group in self.splitBy.keys():\n",
    "            where = \"WHERE {  ?resource info:hasModel 'GenericFile'^^xsd:string ; dcterm:type ?type . filter(?type != 'Thesis'^^xsd:string) . FILTER (contains(str(?resource), '%s'))\" % (self.splitBy[group])\n",
    "            construct = self.construct\n",
    "            for pair in self.mapping:\n",
    "                construct = \"%s ; <%s> ?%s\" % (construct, pair[0], removeNS(pair[0]))\n",
    "                where = \" %s . OPTIONAL { ?resource <%s> ?%s . FILTER (?%s!='') }\" % (where, pair[1], removeNS(pair[0]), removeNS(pair[0]))\n",
    "            self.queries[group] = \"%s %s } %s }\" % (self.prefixes, construct, where)\n",
    "        self.writeQueries()\n",
    "\n",
    "\n",
    "class Thesis(Query):\n",
    "    def __init__(self, sparqlData):\n",
    "        self.ptype = 'thesis'\n",
    "        self.construct = \"CONSTRUCT { ?resource info:hasModel 'IRItem'^^xsd:string ; rdf:type pcdm:Object; rdf:type works:work ; rdf:type bibo:Thesis\"\n",
    "        self.where = []\n",
    "        self.select = \"SELECT distinct ?resource WHERE { ?resource info:hasModel 'GenericFile'^^xsd:string ; dcterm:type 'Thesis'^^xsd:string }\"\n",
    "        super().__init__(self.ptype, sparqlData)\n",
    "\n",
    "    def generateQueries(self):\n",
    "        self.getSplitBy()        \n",
    "        query = \"%s %s\" % (self.prefixes, self.select)\n",
    "        for group in self.splitBy.keys():\n",
    "            where = \"WHERE { ?resource info:hasModel 'GenericFile'^^xsd:string ; dcterm:type 'Thesis'^^xsd:string . FILTER (contains(str(?resource), '%s'))\" % (self.splitBy[group])\n",
    "            construct = self.construct\n",
    "            for pair in self.mapping:\n",
    "                construct = \"%s ; <%s> ?%s\" % (construct, pair[0], removeNS(pair[0]))\n",
    "                where = \" %s . OPTIONAL { ?resource <%s> ?%s . FILTER (?%s!='') }\" % (where, pair[1], removeNS(pair[0]), removeNS(pair[0]))\n",
    "                self.queries[group] =  \"%s %s } %s  }\" % (self.prefixes, construct, where)\n",
    "        self.writeQueries()\n",
    "\n",
    "\n",
    "class Batch(Query):\n",
    "    def __init__(self, sparqlData):\n",
    "        self.ptype = 'batch'\n",
    "        self.construct = \"CONSTRUCT { ?resource info:hasModel 'Batch' ; schema:result ?type; ?predicate ?object }\"\n",
    "        self.where = \"WHERE { ?resource info:hasModel 'Batch'^^xsd:string ; \"\n",
    "        self.select = \"SELECT distinct ?resource WHERE  { ?resource info:hasModel 'Batch'^^xsd:string } \"\n",
    "        super().__init__(self.ptype, sparqlData)\n",
    "    \n",
    "    def generateQueries(self):\n",
    "        self.getSplitBy()\n",
    "        for group in self.splitBy.keys():\n",
    "            self.queries[group] = \"%s %s %s FILTER (contains(str(?resource), '%s')) . ?resource dcterm:type ?type . ?resource ?predicate ?object . FILTER (?object!='') }\" % (self.prefixes, self.construct, self.where, self.splitBy[group])\n",
    "        self.writeQueries()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DATA TRANSPORT OBJECTS\n",
    "##### Runs a query, sends data to get transformed, saves data to appropriate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    def __init__(self, query, group, sparqlData, sparqlTerms, queryObject):\n",
    "        self.q = query\n",
    "        self.group = group\n",
    "        self.sparqlData = sparqlData\n",
    "        self.sparqlTerms = sparqlTerms\n",
    "        self.output = []\n",
    "        self.streamOut = []\n",
    "        self.ptype = queryObject.ptype\n",
    "        self.directory = \"results/%s/\" % (self.ptype)\n",
    "        self.filename = \"results/%s/%s.nt\" % (self.ptype, group)\n",
    "        if not os.path.exists(self.directory):\n",
    "            os.makedirs(self.directory)\n",
    "        \n",
    "\n",
    "    def transformData(self):\n",
    "        self.sparqlData.setMethod(\"GET\")\n",
    "        self.sparqlData.setReturnFormat(JSON)\n",
    "        self.sparqlData.setQuery(self.q)\n",
    "        # queries a batch of resources from this particular \"group\"\n",
    "        results = self.sparqlData.query().convert()['results']['bindings']\n",
    "        # iterates over each resource and performs transformations\n",
    "        for result in results:\n",
    "            result = TransformationFactory().getTransformation(result, self.ptype)\n",
    "            if isinstance(result, list):\n",
    "                for triple in result:\n",
    "                    s = \"<%s>\" % (str(triple['subject']['value']))\n",
    "                    p = \"<%s>\" % (str(triple['predicate']['value']))\n",
    "                    if triple['object']['type'] == 'uri':\n",
    "                        o = \"<%s>\" % (str(triple['object']['value']))\n",
    "                    else:\n",
    "                        o = \"\\\"%s\\\"\" % (str(triple['object']['value']))\n",
    "                    self.streamOut.append(\"%s %s %s . \\n\" % (s, p, o))\n",
    "        with open(self.filename, \"w+\") as f:\n",
    "            f.writelines(self.streamOut)\n",
    "\n",
    "class CollectionData(Data):\n",
    "    def __init__(self, q, group, sparqlData, sparqlTerms, ptype):\n",
    "        super().__init__(q, group, sparqlData, sparqlTerms, ptype)\n",
    "\n",
    "class CommunityData(Data):\n",
    "    def __init__(self, q, group, sparqlData, sparqlTerms, ptype):\n",
    "        super().__init__(q, group, sparqlData, sparqlTerms, ptype)\n",
    "\n",
    "class ThesisData(Data):\n",
    "    def __init__(self, q, group, sparqlData, sparqlTerms, ptype):\n",
    "        super().__init__(q, group, sparqlData, sparqlTerms, ptype)             \n",
    "\n",
    "class GenericData(Data):\n",
    "    def __init__(self, q, group, sparqlData, sparqlTerms, ptype):\n",
    "        super().__init__(q, group, sparqlData, sparqlTerms, ptype)        \n",
    "\n",
    "class BatchData(Data):\n",
    "    def __init__(self, q, group, sparqlData, sparqlTerms, ptype):\n",
    "        super().__init__(q, group, sparqlData, sparqlTerms, ptype)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryFactory():\n",
    "    @staticmethod\n",
    "    def getMigrationQuery(ptype, sparqlData):\n",
    "        \"\"\" returns a specified query object depending on the type passed in\"\"\"\n",
    "        if ptype == \"collection\": return Collection(sparqlData)\n",
    "        elif ptype == \"community\": return Community(sparqlData) \n",
    "        elif ptype == \"thesis\": return Thesis(sparqlData)\n",
    "        elif ptype == \"generic\": return Generic(sparqlData)\n",
    "        elif ptype == \"batch\": return Batch(sparqlData)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFactory():\n",
    "    @staticmethod\n",
    "    def getData(query, group, queryObject):\n",
    "        \"\"\" returns a specified query object depending on the type passed in\"\"\"\n",
    "        if queryObject.ptype == \"collection\": return CollectionData(query, group, queryObject.sparqlData, queryObject.sparqlTerms, queryObject) \n",
    "        elif queryObject.ptype == \"community\": return CommunityData(query, group, queryObject.sparqlData, queryObject.sparqlTerms, queryObject) \n",
    "        elif queryObject.ptype == \"thesis\": return ThesisData(query, group, queryObject.sparqlData, queryObject.sparqlTerms, queryObject)\n",
    "        elif queryObject.ptype == \"generic\": return GenericData(query, group, queryObject.sparqlData, queryObject.sparqlTerms, queryObject)\n",
    "        elif queryObject.ptype == \"batch\": return BatchData(query, group, queryObject.sparqlData, queryObject.sparqlTerms, queryObject)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransformationFactory():\n",
    "    @staticmethod\n",
    "    def getTransformation(triple, ptype):\n",
    "        function = re.sub(r'[0-9]+', '', triple['predicate']['value'].split('/')[-1].replace('#', '').replace('-', ''))\n",
    "        if function == \"rdfsyntaxnstype\": return Transformation().rdfsyntaxnstype(triple, ptype)\n",
    "        elif function == \"language\": return Transformation().language(triple, ptype)\n",
    "        elif function == \"type\": return Transformation().type(triple, ptype)\n",
    "        elif function ==  \"rights\": return Transformation().rights(triple, ptype)\n",
    "        elif function == \"license\": return Transformation().license(triple, ptype)\n",
    "        else:\n",
    "            return [triple]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collection batch queries generated\n",
      "1 batch(es) of collection objects to be transformed\n",
      "1 of 1 collection batches transformed\n",
      "collection objects transformation completed\n",
      "community batch queries generated\n",
      "1 batch(es) of community objects to be transformed\n",
      "1 of 1 community batches transformed\n",
      "community objects transformation completed\n",
      "generic batch queries generated\n",
      "248 batch(es) of generic objects to be transformed\n",
      "1 of 248 generic batches transformed\n",
      "2 of 248 generic batches transformed\n",
      "3 of 248 generic batches transformed\n",
      "4 of 248 generic batches transformed\n",
      "5 of 248 generic batches transformed\n",
      "6 of 248 generic batches transformed\n",
      "7 of 248 generic batches transformed\n",
      "8 of 248 generic batches transformed\n",
      "9 of 248 generic batches transformed\n",
      "10 of 248 generic batches transformed\n",
      "11 of 248 generic batches transformed\n",
      "12 of 248 generic batches transformed\n",
      "13 of 248 generic batches transformed\n",
      "14 of 248 generic batches transformed\n",
      "15 of 248 generic batches transformed\n",
      "16 of 248 generic batches transformed\n",
      "17 of 248 generic batches transformed\n",
      "18 of 248 generic batches transformed\n",
      "19 of 248 generic batches transformed\n",
      "20 of 248 generic batches transformed\n",
      "21 of 248 generic batches transformed\n",
      "22 of 248 generic batches transformed\n",
      "23 of 248 generic batches transformed\n",
      "24 of 248 generic batches transformed\n",
      "25 of 248 generic batches transformed\n",
      "26 of 248 generic batches transformed\n",
      "27 of 248 generic batches transformed\n",
      "28 of 248 generic batches transformed\n",
      "29 of 248 generic batches transformed\n",
      "30 of 248 generic batches transformed\n",
      "31 of 248 generic batches transformed\n",
      "32 of 248 generic batches transformed\n",
      "33 of 248 generic batches transformed\n",
      "34 of 248 generic batches transformed\n",
      "35 of 248 generic batches transformed\n",
      "36 of 248 generic batches transformed\n",
      "37 of 248 generic batches transformed\n",
      "38 of 248 generic batches transformed\n",
      "39 of 248 generic batches transformed\n",
      "40 of 248 generic batches transformed\n",
      "41 of 248 generic batches transformed\n",
      "42 of 248 generic batches transformed\n",
      "43 of 248 generic batches transformed\n",
      "44 of 248 generic batches transformed\n",
      "45 of 248 generic batches transformed\n",
      "46 of 248 generic batches transformed\n",
      "47 of 248 generic batches transformed\n",
      "48 of 248 generic batches transformed\n",
      "49 of 248 generic batches transformed\n",
      "50 of 248 generic batches transformed\n",
      "51 of 248 generic batches transformed\n",
      "52 of 248 generic batches transformed\n",
      "53 of 248 generic batches transformed\n",
      "54 of 248 generic batches transformed\n",
      "55 of 248 generic batches transformed\n",
      "56 of 248 generic batches transformed\n",
      "57 of 248 generic batches transformed\n",
      "58 of 248 generic batches transformed\n",
      "59 of 248 generic batches transformed\n",
      "60 of 248 generic batches transformed\n",
      "61 of 248 generic batches transformed\n",
      "62 of 248 generic batches transformed\n",
      "63 of 248 generic batches transformed\n",
      "64 of 248 generic batches transformed\n",
      "65 of 248 generic batches transformed\n",
      "66 of 248 generic batches transformed\n",
      "67 of 248 generic batches transformed\n",
      "68 of 248 generic batches transformed\n",
      "69 of 248 generic batches transformed\n",
      "70 of 248 generic batches transformed\n",
      "71 of 248 generic batches transformed\n",
      "72 of 248 generic batches transformed\n",
      "73 of 248 generic batches transformed\n",
      "74 of 248 generic batches transformed\n",
      "75 of 248 generic batches transformed\n",
      "76 of 248 generic batches transformed\n",
      "77 of 248 generic batches transformed\n",
      "78 of 248 generic batches transformed\n",
      "79 of 248 generic batches transformed\n",
      "80 of 248 generic batches transformed\n",
      "81 of 248 generic batches transformed\n",
      "82 of 248 generic batches transformed\n",
      "83 of 248 generic batches transformed\n",
      "84 of 248 generic batches transformed\n",
      "85 of 248 generic batches transformed\n",
      "86 of 248 generic batches transformed\n",
      "87 of 248 generic batches transformed\n",
      "88 of 248 generic batches transformed\n",
      "89 of 248 generic batches transformed\n",
      "90 of 248 generic batches transformed\n",
      "91 of 248 generic batches transformed\n",
      "92 of 248 generic batches transformed\n",
      "93 of 248 generic batches transformed\n",
      "94 of 248 generic batches transformed\n",
      "95 of 248 generic batches transformed\n",
      "96 of 248 generic batches transformed\n",
      "97 of 248 generic batches transformed\n",
      "98 of 248 generic batches transformed\n",
      "99 of 248 generic batches transformed\n",
      "100 of 248 generic batches transformed\n",
      "101 of 248 generic batches transformed\n",
      "102 of 248 generic batches transformed\n",
      "103 of 248 generic batches transformed\n",
      "104 of 248 generic batches transformed\n",
      "105 of 248 generic batches transformed\n",
      "106 of 248 generic batches transformed\n",
      "107 of 248 generic batches transformed\n",
      "108 of 248 generic batches transformed\n",
      "109 of 248 generic batches transformed\n",
      "110 of 248 generic batches transformed\n",
      "111 of 248 generic batches transformed\n",
      "112 of 248 generic batches transformed\n",
      "113 of 248 generic batches transformed\n",
      "114 of 248 generic batches transformed\n",
      "115 of 248 generic batches transformed\n",
      "116 of 248 generic batches transformed\n",
      "117 of 248 generic batches transformed\n",
      "118 of 248 generic batches transformed\n",
      "119 of 248 generic batches transformed\n",
      "120 of 248 generic batches transformed\n",
      "121 of 248 generic batches transformed\n",
      "122 of 248 generic batches transformed\n",
      "123 of 248 generic batches transformed\n",
      "124 of 248 generic batches transformed\n",
      "125 of 248 generic batches transformed\n",
      "126 of 248 generic batches transformed\n",
      "127 of 248 generic batches transformed\n",
      "128 of 248 generic batches transformed\n",
      "129 of 248 generic batches transformed\n",
      "130 of 248 generic batches transformed\n",
      "131 of 248 generic batches transformed\n",
      "132 of 248 generic batches transformed\n",
      "133 of 248 generic batches transformed\n",
      "134 of 248 generic batches transformed\n",
      "135 of 248 generic batches transformed\n",
      "136 of 248 generic batches transformed\n",
      "137 of 248 generic batches transformed\n",
      "138 of 248 generic batches transformed\n",
      "139 of 248 generic batches transformed\n",
      "140 of 248 generic batches transformed\n",
      "141 of 248 generic batches transformed\n",
      "142 of 248 generic batches transformed\n",
      "143 of 248 generic batches transformed\n",
      "144 of 248 generic batches transformed\n",
      "145 of 248 generic batches transformed\n",
      "146 of 248 generic batches transformed\n",
      "147 of 248 generic batches transformed\n",
      "148 of 248 generic batches transformed\n",
      "149 of 248 generic batches transformed\n",
      "150 of 248 generic batches transformed\n",
      "151 of 248 generic batches transformed\n",
      "152 of 248 generic batches transformed\n",
      "153 of 248 generic batches transformed\n",
      "154 of 248 generic batches transformed\n",
      "155 of 248 generic batches transformed\n",
      "156 of 248 generic batches transformed\n",
      "157 of 248 generic batches transformed\n",
      "158 of 248 generic batches transformed\n",
      "159 of 248 generic batches transformed\n",
      "160 of 248 generic batches transformed\n",
      "161 of 248 generic batches transformed\n",
      "162 of 248 generic batches transformed\n",
      "163 of 248 generic batches transformed\n",
      "164 of 248 generic batches transformed\n",
      "165 of 248 generic batches transformed\n",
      "166 of 248 generic batches transformed\n",
      "167 of 248 generic batches transformed\n",
      "168 of 248 generic batches transformed\n",
      "169 of 248 generic batches transformed\n",
      "170 of 248 generic batches transformed\n",
      "171 of 248 generic batches transformed\n",
      "172 of 248 generic batches transformed\n",
      "173 of 248 generic batches transformed\n",
      "174 of 248 generic batches transformed\n",
      "175 of 248 generic batches transformed\n",
      "176 of 248 generic batches transformed\n",
      "177 of 248 generic batches transformed\n",
      "178 of 248 generic batches transformed\n",
      "179 of 248 generic batches transformed\n",
      "180 of 248 generic batches transformed\n",
      "181 of 248 generic batches transformed\n",
      "182 of 248 generic batches transformed\n",
      "183 of 248 generic batches transformed\n",
      "184 of 248 generic batches transformed\n",
      "185 of 248 generic batches transformed\n",
      "186 of 248 generic batches transformed\n",
      "187 of 248 generic batches transformed\n",
      "188 of 248 generic batches transformed\n",
      "189 of 248 generic batches transformed\n",
      "190 of 248 generic batches transformed\n",
      "191 of 248 generic batches transformed\n",
      "192 of 248 generic batches transformed\n",
      "193 of 248 generic batches transformed\n",
      "194 of 248 generic batches transformed\n",
      "195 of 248 generic batches transformed\n",
      "196 of 248 generic batches transformed\n",
      "197 of 248 generic batches transformed\n",
      "198 of 248 generic batches transformed\n",
      "199 of 248 generic batches transformed\n",
      "200 of 248 generic batches transformed\n",
      "201 of 248 generic batches transformed\n",
      "202 of 248 generic batches transformed\n",
      "203 of 248 generic batches transformed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 of 248 generic batches transformed\n",
      "205 of 248 generic batches transformed\n",
      "206 of 248 generic batches transformed\n",
      "207 of 248 generic batches transformed\n",
      "208 of 248 generic batches transformed\n",
      "209 of 248 generic batches transformed\n",
      "210 of 248 generic batches transformed\n",
      "211 of 248 generic batches transformed\n",
      "212 of 248 generic batches transformed\n",
      "213 of 248 generic batches transformed\n",
      "214 of 248 generic batches transformed\n",
      "215 of 248 generic batches transformed\n",
      "216 of 248 generic batches transformed\n",
      "217 of 248 generic batches transformed\n",
      "218 of 248 generic batches transformed\n",
      "219 of 248 generic batches transformed\n",
      "220 of 248 generic batches transformed\n",
      "221 of 248 generic batches transformed\n",
      "222 of 248 generic batches transformed\n",
      "223 of 248 generic batches transformed\n",
      "224 of 248 generic batches transformed\n",
      "225 of 248 generic batches transformed\n",
      "226 of 248 generic batches transformed\n",
      "227 of 248 generic batches transformed\n",
      "228 of 248 generic batches transformed\n",
      "229 of 248 generic batches transformed\n",
      "230 of 248 generic batches transformed\n",
      "231 of 248 generic batches transformed\n",
      "232 of 248 generic batches transformed\n",
      "233 of 248 generic batches transformed\n",
      "234 of 248 generic batches transformed\n",
      "235 of 248 generic batches transformed\n",
      "236 of 248 generic batches transformed\n",
      "237 of 248 generic batches transformed\n",
      "238 of 248 generic batches transformed\n",
      "239 of 248 generic batches transformed\n",
      "240 of 248 generic batches transformed\n",
      "241 of 248 generic batches transformed\n",
      "242 of 248 generic batches transformed\n",
      "243 of 248 generic batches transformed\n",
      "244 of 248 generic batches transformed\n",
      "245 of 248 generic batches transformed\n",
      "246 of 248 generic batches transformed\n",
      "247 of 248 generic batches transformed\n",
      "248 of 248 generic batches transformed\n",
      "generic objects transformation completed\n",
      "thesis batch queries generated\n",
      "193 batch(es) of thesis objects to be transformed\n",
      "1 of 193 thesis batches transformed\n",
      "2 of 193 thesis batches transformed\n",
      "3 of 193 thesis batches transformed\n",
      "4 of 193 thesis batches transformed\n",
      "5 of 193 thesis batches transformed\n",
      "6 of 193 thesis batches transformed\n",
      "7 of 193 thesis batches transformed\n",
      "8 of 193 thesis batches transformed\n",
      "9 of 193 thesis batches transformed\n",
      "10 of 193 thesis batches transformed\n",
      "11 of 193 thesis batches transformed\n",
      "12 of 193 thesis batches transformed\n",
      "13 of 193 thesis batches transformed\n",
      "14 of 193 thesis batches transformed\n",
      "15 of 193 thesis batches transformed\n",
      "16 of 193 thesis batches transformed\n",
      "17 of 193 thesis batches transformed\n",
      "18 of 193 thesis batches transformed\n",
      "19 of 193 thesis batches transformed\n",
      "20 of 193 thesis batches transformed\n",
      "21 of 193 thesis batches transformed\n",
      "22 of 193 thesis batches transformed\n",
      "23 of 193 thesis batches transformed\n",
      "24 of 193 thesis batches transformed\n",
      "25 of 193 thesis batches transformed\n",
      "26 of 193 thesis batches transformed\n",
      "27 of 193 thesis batches transformed\n",
      "28 of 193 thesis batches transformed\n",
      "29 of 193 thesis batches transformed\n",
      "30 of 193 thesis batches transformed\n",
      "31 of 193 thesis batches transformed\n",
      "32 of 193 thesis batches transformed\n",
      "33 of 193 thesis batches transformed\n",
      "34 of 193 thesis batches transformed\n",
      "35 of 193 thesis batches transformed\n",
      "36 of 193 thesis batches transformed\n",
      "37 of 193 thesis batches transformed\n",
      "38 of 193 thesis batches transformed\n",
      "39 of 193 thesis batches transformed\n",
      "40 of 193 thesis batches transformed\n",
      "41 of 193 thesis batches transformed\n",
      "42 of 193 thesis batches transformed\n",
      "43 of 193 thesis batches transformed\n",
      "44 of 193 thesis batches transformed\n",
      "45 of 193 thesis batches transformed\n",
      "46 of 193 thesis batches transformed\n",
      "47 of 193 thesis batches transformed\n",
      "48 of 193 thesis batches transformed\n",
      "49 of 193 thesis batches transformed\n",
      "50 of 193 thesis batches transformed\n",
      "51 of 193 thesis batches transformed\n",
      "52 of 193 thesis batches transformed\n",
      "53 of 193 thesis batches transformed\n",
      "54 of 193 thesis batches transformed\n",
      "55 of 193 thesis batches transformed\n",
      "56 of 193 thesis batches transformed\n",
      "57 of 193 thesis batches transformed\n",
      "58 of 193 thesis batches transformed\n",
      "59 of 193 thesis batches transformed\n",
      "60 of 193 thesis batches transformed\n",
      "61 of 193 thesis batches transformed\n",
      "62 of 193 thesis batches transformed\n",
      "63 of 193 thesis batches transformed\n",
      "64 of 193 thesis batches transformed\n",
      "65 of 193 thesis batches transformed\n",
      "66 of 193 thesis batches transformed\n",
      "67 of 193 thesis batches transformed\n",
      "68 of 193 thesis batches transformed\n",
      "69 of 193 thesis batches transformed\n",
      "70 of 193 thesis batches transformed\n",
      "71 of 193 thesis batches transformed\n",
      "72 of 193 thesis batches transformed\n",
      "73 of 193 thesis batches transformed\n",
      "74 of 193 thesis batches transformed\n",
      "75 of 193 thesis batches transformed\n",
      "76 of 193 thesis batches transformed\n",
      "77 of 193 thesis batches transformed\n",
      "78 of 193 thesis batches transformed\n",
      "79 of 193 thesis batches transformed\n",
      "80 of 193 thesis batches transformed\n",
      "81 of 193 thesis batches transformed\n",
      "82 of 193 thesis batches transformed\n",
      "83 of 193 thesis batches transformed\n",
      "84 of 193 thesis batches transformed\n",
      "85 of 193 thesis batches transformed\n",
      "86 of 193 thesis batches transformed\n",
      "87 of 193 thesis batches transformed\n",
      "88 of 193 thesis batches transformed\n",
      "89 of 193 thesis batches transformed\n",
      "90 of 193 thesis batches transformed\n",
      "91 of 193 thesis batches transformed\n",
      "92 of 193 thesis batches transformed\n",
      "93 of 193 thesis batches transformed\n",
      "94 of 193 thesis batches transformed\n",
      "95 of 193 thesis batches transformed\n",
      "96 of 193 thesis batches transformed\n",
      "97 of 193 thesis batches transformed\n",
      "98 of 193 thesis batches transformed\n",
      "99 of 193 thesis batches transformed\n",
      "100 of 193 thesis batches transformed\n",
      "101 of 193 thesis batches transformed\n",
      "102 of 193 thesis batches transformed\n",
      "103 of 193 thesis batches transformed\n",
      "104 of 193 thesis batches transformed\n",
      "105 of 193 thesis batches transformed\n",
      "106 of 193 thesis batches transformed\n",
      "107 of 193 thesis batches transformed\n",
      "108 of 193 thesis batches transformed\n",
      "109 of 193 thesis batches transformed\n",
      "110 of 193 thesis batches transformed\n",
      "111 of 193 thesis batches transformed\n",
      "112 of 193 thesis batches transformed\n",
      "113 of 193 thesis batches transformed\n",
      "114 of 193 thesis batches transformed\n",
      "115 of 193 thesis batches transformed\n",
      "116 of 193 thesis batches transformed\n",
      "117 of 193 thesis batches transformed\n",
      "118 of 193 thesis batches transformed\n",
      "119 of 193 thesis batches transformed\n",
      "120 of 193 thesis batches transformed\n",
      "121 of 193 thesis batches transformed\n",
      "122 of 193 thesis batches transformed\n",
      "123 of 193 thesis batches transformed\n",
      "124 of 193 thesis batches transformed\n",
      "125 of 193 thesis batches transformed\n",
      "126 of 193 thesis batches transformed\n",
      "127 of 193 thesis batches transformed\n",
      "128 of 193 thesis batches transformed\n",
      "129 of 193 thesis batches transformed\n",
      "130 of 193 thesis batches transformed\n",
      "131 of 193 thesis batches transformed\n",
      "132 of 193 thesis batches transformed\n",
      "133 of 193 thesis batches transformed\n",
      "134 of 193 thesis batches transformed\n",
      "135 of 193 thesis batches transformed\n",
      "136 of 193 thesis batches transformed\n",
      "137 of 193 thesis batches transformed\n",
      "138 of 193 thesis batches transformed\n",
      "139 of 193 thesis batches transformed\n",
      "140 of 193 thesis batches transformed\n",
      "141 of 193 thesis batches transformed\n",
      "142 of 193 thesis batches transformed\n",
      "143 of 193 thesis batches transformed\n",
      "144 of 193 thesis batches transformed\n",
      "145 of 193 thesis batches transformed\n",
      "146 of 193 thesis batches transformed\n",
      "147 of 193 thesis batches transformed\n",
      "148 of 193 thesis batches transformed\n",
      "149 of 193 thesis batches transformed\n",
      "150 of 193 thesis batches transformed\n",
      "151 of 193 thesis batches transformed\n",
      "152 of 193 thesis batches transformed\n",
      "153 of 193 thesis batches transformed\n",
      "154 of 193 thesis batches transformed\n",
      "155 of 193 thesis batches transformed\n",
      "156 of 193 thesis batches transformed\n",
      "157 of 193 thesis batches transformed\n",
      "158 of 193 thesis batches transformed\n",
      "159 of 193 thesis batches transformed\n",
      "160 of 193 thesis batches transformed\n",
      "161 of 193 thesis batches transformed\n",
      "162 of 193 thesis batches transformed\n",
      "163 of 193 thesis batches transformed\n",
      "164 of 193 thesis batches transformed\n",
      "165 of 193 thesis batches transformed\n",
      "166 of 193 thesis batches transformed\n",
      "167 of 193 thesis batches transformed\n",
      "168 of 193 thesis batches transformed\n",
      "169 of 193 thesis batches transformed\n",
      "170 of 193 thesis batches transformed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171 of 193 thesis batches transformed\n",
      "172 of 193 thesis batches transformed\n",
      "173 of 193 thesis batches transformed\n",
      "174 of 193 thesis batches transformed\n",
      "175 of 193 thesis batches transformed\n",
      "176 of 193 thesis batches transformed\n",
      "177 of 193 thesis batches transformed\n",
      "178 of 193 thesis batches transformed\n",
      "179 of 193 thesis batches transformed\n",
      "180 of 193 thesis batches transformed\n",
      "181 of 193 thesis batches transformed\n",
      "182 of 193 thesis batches transformed\n",
      "183 of 193 thesis batches transformed\n",
      "184 of 193 thesis batches transformed\n",
      "185 of 193 thesis batches transformed\n",
      "186 of 193 thesis batches transformed\n",
      "187 of 193 thesis batches transformed\n",
      "188 of 193 thesis batches transformed\n",
      "189 of 193 thesis batches transformed\n",
      "190 of 193 thesis batches transformed\n",
      "191 of 193 thesis batches transformed\n",
      "192 of 193 thesis batches transformed\n",
      "193 of 193 thesis batches transformed\n",
      "thesis objects transformation completed\n",
      "batch batch queries generated\n",
      "9 batch(es) of batch objects to be transformed\n",
      "1 of 9 batch batches transformed\n",
      "2 of 9 batch batches transformed\n",
      "3 of 9 batch batches transformed\n",
      "4 of 9 batch batches transformed\n",
      "5 of 9 batch batches transformed\n",
      "6 of 9 batch batches transformed\n",
      "7 of 9 batch batches transformed\n",
      "8 of 9 batch batches transformed\n",
      "9 of 9 batch batches transformed\n",
      "batch objects transformation completed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
