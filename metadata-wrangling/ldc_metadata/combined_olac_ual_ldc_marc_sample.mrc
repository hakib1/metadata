05028nmm a22004213i 4500006001900000007001500019008004100034020001500075024001500090024002900105041002300134041003300157042000700190245005600197264007300253336003800326336002600364337002600390338003200416500001500448500004500463506005100508520325100559520005103810546008703861650005603948650005204004650005004056655002004106700005804126700001204184700004904196700005704245700006604302700002104368856014904389856006804538m     o  u        cu ||||||u||||      s2018    pau        u        gre d  a15856382858 aLDC2018T078 aISLRN: 270-733-242-642-30 agreahunaitabeng07aellahunaitabeng2iso639-3  adc00a2007 CoNLL Shared Task - Greek, Hungarian & Italian  a[Philadelphia, Pennsylvania]: bLinguistic Data Consortium, c[2018]  acomputer datasetbcod2rdacontent  atextbtxt2rdacontent  acomputerbc2rdamedia  aunspecifiedbzu2rdacarrier  aLDC2018T07  ahttps://catalog.ldc.upenn.edu/LDC2018T071 aAvailable to University of Alberta users only.  a2007 CoNLL Shared Task - Greek, Hungarian & Italian consists of dependency treebanks in three languages used as part of the CoNLL 2007 shared task on multi-lingual dependency parsing and domain adaptation. The languages covered in this release are: Greek, Hungarian and Italian. LDC also released the following 2006 & 2007 CoNLL Shared Task corpora: * 2007 CoNLL Shared Task - Basque, Catalan, Czech & Turkish (LDC2018T06) * 2007 CoNLL Shared Task - Arabic & English (LDC2018T08) * 2006 CoNLL Shared Task - Ten Languages (LDC2015T11) * 2006 CoNLL Shared Task - 2006 CoNLL Shared Task - Arabic & Czech (LDC2015T12) This corpus is cross listed and jointly released with ELRA as ELRA-W0122. The Conference on Computational Natural Language Learning (CoNLL) is accompanied every year by a shared task intended to promote natural language processing applications and evaluate them in a standard setting. In 2006 and 2007, the shared tasks were devoted to the parsing of syntactic dependencies using corpora from up to thirteen languages. The task aimed to define and extend the then-current state of the art in dependency parsing, a technology that complemented previous tasks by producing a different kind of syntactic description of input text. The 2007 shared task added a domain adaptation track for English in addition to the multilingual track. More information about the 2007 shared task is available at the CoNLL Previous Tasks web site. LDC has released data sets from other CoNLL shared tasks. 2008 CoNLL Shared Task Data (LDC2009T12) contains the English material used in the 2008 shared task which focused on English, employed a unified dependency-based formalism and merged the tasks of syntactic dependency parsing, identifying semantic arguments and labeling them with semantic roles. 2009 CoNLL Shared Task Data Parts 1 and 2 (LDC2012T03 and LDC2012T04) consists of the English, Catalan, Chinese, Czech, German and Spanish resources used in the 2009 task which included a comparison of time and space complexity based on participants' input and learning curve comparison for languages with large datasets. 2015-2016 CoNLL Shared Task (LDC2017T13) contains Chinese and English resources used in the 2015 and 2016 shared tasks on dependency parsing. *Data* The source data in the treebanks in this release consists principally of various texts (e.g., textbooks, news, literature) annotated in dependency format. In general, dependency grammar is based on the idea that the verb is the center of the clause structure and that other units in the sentence are connected to the verb as directed links or dependencies. This is a one-to-one correspondence: for every element in the sentence there is one node in the sentence structure that corresponds to that element. In constituency or phrase structure grammars, on the other hand, clauses are divided into noun phrases and verb phrases and in each sentence, one or more nodes may correspond to one element. The Penn Treebank (LDC99T42) is an example of a constituency or phrase structure approach. All of the data sets in this release are dependency treebanks. The individual data sets are: * Greek Dependency Treebank (Greek) * The Szeged Treebank (SzTB) (Hungarian) * ISST-CoNLL (Italian)  aData samples are available on the LDC website.  aContent in Modern Greek (1453-), Hungarian, and Italian. Documentation in English. 0aGreek language, ModernxData processingvDatabases. 0aHungarian languagexData processingvDatabases. 0aItalian languagexData processingvDatabases.7 aExcerpts2lcgft10aDipartimento di Informatica of the University of Pisa10aILC-CNR10aInstitute for Language and Speech Processing10aInstitute of Informatics at the University of Szeged10aInstitute of Linguistics at the Hungarian Academy of Sciences10aMorphologic Ltd.403University of Alberta Access (Request Form)uhttps://docs.google.com/forms/d/e/1FAIpQLSd4VsEYOWoubQww-01W7IV2qDaAr4ctBJUhrJvfyN0GwoMuFQ/viewform423Dataset documentationuhttps://catalog.ldc.upenn.edu/LDC2018T0702791nmm a22004573i 4500006001900000007001500019008004100034024001500075024002900090041001800119041002800137042000700165245004600172264007300218336003800291336005000329336003300379337002600412338003200438500001500470500004500485506005100530520100300581520005101584546006201635650005001697650006601747650005001813650006601863655002801929655002801957700001501985700002202000700001902022700001802041700002202059700001802081700001702099856014902116856006802265m     o  u        cu ||||||u||||      s2017    pau        u        eng d8 aLDC2017V018 aISLRN: 810-731-329-467-50 aengachibeng07aengazhobeng2iso639-3  adc00aUCLA High-Speed Laryngeal Video and Audio  a[Philadelphia, Pennsylvania]: bLinguistic Data Consortium, c[2017]  acomputer datasetbcod2rdacontent  atwo-dimensional moving imagebtdi2rdacontent  aspoken wordbspw2rdacontent  acomputerbc2rdamedia  aunspecifiedbzu2rdacarrier  aLDC2017V01  ahttps://catalog.ldc.upenn.edu/LDC2017V011 aAvailable to University of Alberta users only.  aUCLA High-Speed Laryngeal Video and Audio was developed by UCLA Speech Processing and Auditory Perception Laboratory and is comprised of high-speed laryngeal video recordings of the vocal folds and synchronized audio recordings from nine subjects collected between April 2012 and April 2013. Speakers were asked to sustain the vowel /i/ for approximately ten seconds while holding voice quality, fundamental frequency, and loudness as steady as possible. In the field of speech production theory, data such as contained in this release may be used to study the relationship between vocal folds vibration and resulting voice quality. *Data* None of the subjects had a history of a voice disorder. There was no native language requirement for recruiting subjects; participants were native speakers of various languages, including English, Mandarin Chinese, Taiwanese Mandarin, Cantonese and German. Audio data is presented as 16kHz 16-bit flac and video is in avi format at 5 fps (frames per second).  aData samples are available on the LDC website.  aContent in English and Chinese. Documentation in English. 0aEnglish languagexData processingvDatabases. 0aEnglish languagexSpoken EnglishxData processingvDatabases. 0aChinese languagexData processingvDatabases. 0aChinese languagexSpoken ChinesexData processingvDatabases.7 aVideo recordings2lcgft7 aSound recordings2lcgft10aChen, Gang10aNeubauer, Juergen10aGarellek, Marc10aSamlan, Robin10aGerratt, Bruce R.10aKreiman, Jody10aAlwan, Abeer403University of Alberta Access (Request Form)uhttps://docs.google.com/forms/d/e/1FAIpQLSd4VsEYOWoubQww-01W7IV2qDaAr4ctBJUhrJvfyN0GwoMuFQ/viewform423Dataset documentationuhttps://catalog.ldc.upenn.edu/LDC2017V0104860nmm a22005173i 4500006001900000007001500019008004100034020001500075024001500090024002900105041003300134041004800167042000700215245003300222264007300255336003800328336003300366337002600399338003200425500001500457500004500472506005100517520257800568520005103146546012203197650005503319650007603374650004903450650006403499650005003563650006603613650004703679650006003726650004903786650006403835650004703899650006003946655002804006700001704034700001504051700002404066700001804090700001704108856014904125856006804274m     o  u        cu ||||||u||||      s2018    pau        u        sem d  a15856385288 aLDC2018S108 aISLRN: 190-505-311-077-00 asemaaraaperapusaurdbeng07aajpaapcafasaprsapusaurdbeng2iso639-3  adc00aRATS Language Identification  a[Philadelphia, Pennsylvania]: bLinguistic Data Consortium, c[2018]  acomputer datasetbcod2rdacontent  aspoken wordbspw2rdacontent  acomputerbc2rdamedia  aunspecifiedbzu2rdacarrier  aLDC2018S10  ahttps://catalog.ldc.upenn.edu/LDC2018S101 aAvailable to University of Alberta users only.  aRATS Language Identification was developed by the Linguistic Data Consortium (LDC) and is comprised of approximately 5,400 hours of Levantine Arabic, Farsi, Dari, Pashto and Urdu conversational telephone speech with annotation of speech segments. The corpus was created to provide training, development and initial test sets for the Language Identification (LID) task in the DARPA RATS (Robust Automatic Transcription of Speech) program. The goal of the RATS program was to develop human language technology systems capable of performing speech detection, language identification, speaker identification and keyword spotting on the severely degraded audio signals that are typical of various radio communication channels, especially those employing various types of handheld portable transceiver systems. To support that goal, LDC assembled a system for the transmission, reception and digital capture of audio data that allowed a single source audio signal to be distributed and recorded over eight distinct transceiver configurations simultaneously. Those configurations included three frequencies -- high, very high and ultra high -- variously combined with amplitude modulation, frequency hopping spread spectrum, narrow-band frequency modulation, single-side-band or wide-band frequency modulation. Annotations on the clear source audio signal, e.g., time boundaries for the duration of speech activity, were projected onto the corresponding eight channels recorded from the radio receivers. *Data* The source audio consists of conversational telephone speech recordings from: (1) conversational telephone speech (CTS) recordings, taken either from previous LDC CTS corpora, or from CTS data collected specifically for the RATS program from Levantine Arabic, Pashto, Urdu, Farsi and Dari native speakers; and (2) portions of VOA broadcast news recordings, taken from data used in the 2009 NIST Language Recognition Evaluation. The 2009 LRE Test Set is available from LDC as LDC2014S06. CTS recordings were audited by annotators who listened to short segments and determined whether the audio was in the target language. Annotations on the audio files include start time, end time, speech activity detection (SAD) label, SAD provenance, language ID and LID provenance. All audio files are presented as single-channel, 16-bit PCM, 16000 samples per second; lossless FLAC compression is used on all files; when uncompressed, the files have typical "MS-WAV" (RIFF) file headers. The data is divided for use as training, initial development set, and initial evaluation set.  aData samples are available on the LDC website.  aContent in South Levantine Arabic, North Levantine Arabic, Persian, Dari, Pushto, and Urdu. Documentation in English. 0aSouth Arabic languagexData processingvDatabases. 0aSouth Arabic languagexSpoken South ArabicxData processingvDatabases. 0aArabic languagexData processingvDatabases. 0aArabic languagexSpoken ArabicxData processingvDatabases. 0aPersian languagexData processingvDatabases. 0aPersian languagexSpoken PersianxData processingvDatabases. 0aDari languagexData processingvDatabases. 0aDari languagexSpoken DarixData processingvDatabases. 0aPushto languagexData processingvDatabases. 0aPushto languagexSpoken PushtoxData processingvDatabases. 0aUrdu languagexData processingvDatabases. 0aUrdu languagexSpoken UrduxData processingvDatabases.7 aSound recordings2lcgft10aGraff, David10aMa, Xiaoyi10aStrassel, Stephanie10aWalker, Kevin10aJones, Karen403University of Alberta Access (Request Form)uhttps://docs.google.com/forms/d/e/1FAIpQLSd4VsEYOWoubQww-01W7IV2qDaAr4ctBJUhrJvfyN0GwoMuFQ/viewform423Dataset documentationuhttps://catalog.ldc.upenn.edu/LDC2018S1003227nmm a22004093i 4500006001900000007001500019008004100034020001500075024001500090024002900105042000700134245003000141264007300171336003800244336005000282336002600332337002600358338003200384500001500416500004500431506005100476520168900527520005102216546004202267650005002309650006602359655002802425655002002453700002102473700002402494700001902518700001702537700002202554700002402576856014902600856006802749m     o  u        cu ||||||u||||      s2016    pau        u        eng d  a15856375138 aLDC2016V018 aISLRN: 609-210-869-474-9  adc00aHAVIC Pilot Transcription  a[Philadelphia, Pennsylvania]: bLinguistic Data Consortium, c[2016]  acomputer datasetbcod2rdacontent  atwo-dimensional moving imagebtdi2rdacontent  atextbtxt2rdacontent  acomputerbc2rdamedia  aunspecifiedbzu2rdacarrier  aLDC2016V01  ahttps://catalog.ldc.upenn.edu/LDC2016V011 aAvailable to University of Alberta users only.  aHAVIC Pilot Transcription was developed by the Linguistic Data Consortium (LDC) and is comprised of approximately 72 hours of user-generated videos with transcripts based on the English speech audio extracted from the videos. This data set was created in collaboration with NIST (the National Institute of Standards and Technology) as part of the HAVIC (the Heterogeneous Audio Visual Internet Collection) project, the goal of which is to advance multimodal event detection and related technologies. LDC has developed a large, heterogeneous, annotated multimodal corpus for HAVIC that has been used in the NIST-sponsored MED (Multimedia Event Detection) task for several years. HAVIC Pilot Transcription supported an experiment to produce a verbatim transcript (quick and rich transcription) based on audio extracted from user-generated videos. It contains the pilot transcripts for selected MED 2011 video files as well as the associated videos. *Data* NIST designated the videos to be transcribed. Annotators generated the transcripts using XTrans, which supports manual transcription across multiple channels, languages and platforms. HAVIC transcription guidelines are included in the documentation for this release. Each file was transcribed by a single annotator with no corpus-wide second pass. File samples from each annotator were checked for various errors, including missing transcription, improper mark-up, poor segmentation and missing/added words. All transcription files are in .tdf format, a plain-text, flat-table format with 13 tab-delimited fields. All video files are in .mp4 format (h264), with varying bit-rates and levels of audio fidelity and video resolution.  aData samples are available on the LDC website.  aContent and documentation in English. 0aEnglish languagexData processingvDatabases. 0aEnglish languagexSpoken EnglishxData processingvDatabases.7 aVideo recordings2lcgft7 aExcerpts2lcgft10aTracey, Jennifer10aStrassel, Stephanie10aMorris, Amanda10aLi, Xuansong10aAntonishek, Brian10aFiscus, Jonathan G.403University of Alberta Access (Request Form)uhttps://docs.google.com/forms/d/e/1FAIpQLSd4VsEYOWoubQww-01W7IV2qDaAr4ctBJUhrJvfyN0GwoMuFQ/viewform423Dataset documentationuhttps://catalog.ldc.upenn.edu/LDC2016V0103826nmm a22004213i 4500006001900000007001500019008004100034020001500075024001500090024002900105041001800134041002800152042000700180245008000187264007300267336003800340336002600378336003800404337002600442338003200468500001500500500004500515506005100560520221900611520005102830546006102881650004902942650005002991655002003041655002803061700002103089700001703110700002403127700001503151700002103166856014903187856006803336m     o  u        cu ||||||u||||      s2018    pau        u        som d  a15856383828 aLDC2018T118 aISLRN: 358-095-625-105-70 asomaengbeng07asomaengbeng2iso639-3  adc00aLORELEI Somali Representative Language Pack - Monolingual and Parallel Text  a[Philadelphia, Pennsylvania]: bLinguistic Data Consortium, c[2018]  acomputer datasetbcod2rdacontent  atextbtxt2rdacontent  acomputer programbcop2rdacontent  acomputerbc2rdamedia  aunspecifiedbzu2rdacarrier  aLDC2018T11  ahttps://catalog.ldc.upenn.edu/LDC2018T111 aAvailable to University of Alberta users only.  aLORELEI Somali Representative Language Pack - Monolingual and Parallel Text was developed by the Linguistic Data Consortium (LDC) and is comprised of approximately 13 million words of monolingual Somali text, approximately 800,000 of which are translated into English. Another 100,000 words are also translated from English into Somali. The LORELEI (Low Resource Languages for Emergent Incidents) Program is concerned with building Human Language Technology for low resource languages in the context of emergent situations like natural disasters or disease outbreaks. Linguistic resources for LORELEI include Representative Language Packs and Incident Language Packs for over two dozen low resource languages, comprising data, annotations, basic natural language processing tools, lexicons and grammatical resources. Representative languages are selected to provide broad typological coverage, while incident languages are selected to evaluate system performance on a language whose identity is disclosed at the start of the evaluation. *Data* Data was collected in the following genres: discussion forums, news, reference, social network and weblog. Both monolingual text collection and parallel text creation involved a combination of manual and automatic methods, which are detailed in the included documentation. All harvested content was initially converted from its original HTML form into a relatively uniform XML format. XML data is presented in two formats: a "homogenized" XML format that preserves the minimum set of tags needed to represent the structure of the relevant text as seen by the human web-page reader and a fully segmented and tokenized version of the text. All text data is encoded as UTF-8. Also included in this release are two tools: one to recreate original source data from the processed XML material and the other to condition text data users download from Twitter. *Acknowledgement* This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-15-C-0123. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of DARPA.  aData samples are available on the LDC website.  aContent in Somali and English. Documentation in English. 0aSomali languagexData processingvDatabases. 0aEnglish languagexData processingvDatabases.7 aExcerpts2lcgft7 acomputer program2lcgft10aTracey, Jennifer10aGraff, David10aStrassel, Stephanie10aMa, Xiaoyi10aWright, Jonathan403University of Alberta Access (Request Form)uhttps://docs.google.com/forms/d/e/1FAIpQLSd4VsEYOWoubQww-01W7IV2qDaAr4ctBJUhrJvfyN0GwoMuFQ/viewform423Dataset documentationuhttps://catalog.ldc.upenn.edu/LDC2018T1102711nmm a22003853i 4500006001900000007001500019008004100034020001500075024001500090024002900105042000700134245001600141264007300157336003800230336003300268337002600301338003200327500001500359500004500374506005100419520127800470520005101748546004201799650005001841650006601891655002801957700002201985700001602007700002102023700002202044700002102066700002102087856014902108856006802257m     o  u        cu ||||||u||||      s2017    pau        u        eng d  a15856380138 aLDC2017S108 aISLRN: 071-714-384-459-0  adc00aCHiME2 WSJ0  a[Philadelphia, Pennsylvania]: bLinguistic Data Consortium, c[2017]  acomputer datasetbcod2rdacontent  aspoken wordbspw2rdacontent  acomputerbc2rdamedia  aunspecifiedbzu2rdacarrier  aLDC2017S10  ahttps://catalog.ldc.upenn.edu/LDC2017S101 aAvailable to University of Alberta users only.  aCHiME2 WSJ0 was developed as part of The 2nd CHiME Speech Separation and Recognition Challenge and contains approximately 166 hours of English speech from a noisy living room environment. The CHiME Challenges focus on distant-microphone automatic speech recognition (ASR) in real-world environments. CHiME2 WSJ0 reflects the medium vocabulary track of the CHiME2 Challenge. The target utterances were taken from CSR-I (WSJ0) Complete (LDC93S6A), specifically, the 5,000 word subset of read speech from Wall Street Journal news text. LDC also released CHiME2 Grid (LDC2017S07) and CHiME3 (LDC2017S24). *Data* Data is divided into training, development and test sets. All data is provided as 16 bit WAV files sampled at 16 kHz. The noisy utterances are in isolated form and in embedded form. The latter involves five seconds of background noise before and after the utterance. Seven hours of noise background not part of the training set are also included. Also included are baseline scoring, decoding and retraining tools based on Cambridge University' s tool, HTK (the Hidden Markov Toolkit) and related recipes. These tools include three baseline speaker-independent recognition systems trained on clean, reverberated and noisy data, respectively, and a number of scripts.  aData samples are available on the LDC website.  aContent and documentation in English. 0aEnglish languagexData processingvDatabases. 0aEnglish languagexSpoken EnglishxData processingvDatabases.7 aSound recordings2lcgft10aVincent, Emmanuel10aBarker, Jon10aWatanabe, Shinji10aLe Roux, Jonathan10aNesta, Francesco10aMatassoni, Marco403University of Alberta Access (Request Form)uhttps://docs.google.com/forms/d/e/1FAIpQLSd4VsEYOWoubQww-01W7IV2qDaAr4ctBJUhrJvfyN0GwoMuFQ/viewform423Dataset documentationuhttps://catalog.ldc.upenn.edu/LDC2017S1003013nmm a22003493i 4500006001900000007001500019008004100034020001500075024001300090024002900103041002300132041003300155042000700188245003700195264007300232336003800305336002600343337002600369338003200395500001300427500004300440506005100483520165300534546007302187650005302260650004902313650004902362655002002411700001702431856014902448856006602597m     o  u        cu ||||||u||||      s1995    pau        u        por d  a15856304708 aLDC95T118 aISLRN: 082-576-700-069-20 aporafreagerbeng07aporafraadeubeng2iso639-3  adc00aEuropean Language Newspaper Text  a[Philadelphia, Pennsylvania]: bLinguistic Data Consortium, c[1995]  acomputer datasetbcod2rdacontent  atextbtxt2rdacontent  acomputerbc2rdamedia  aunspecifiedbzu2rdacarrier  aLDC95T11  ahttps://catalog.ldc.upenn.edu/LDC95T111 aAvailable to University of Alberta users only.  aThe European Language Newspaper Text corpus is also know as the French Language News Corpus. This corpus includes roughly 100 million words of French, 90 million words of German and 15 million words of Portuguese and has been marked using SGML. The text is taken from the following sources: * Approximately 60 million words of text in French and German have been made available from the Associated Press (AP) World Stream. AP World Stream is a compilation of AP news reports produced in 86 bureaus in 68 countries. The Associated Press Worldstream newswire service provides articles in six languages, interleaved on a single data stream. The data is collected via an Associated Press installed telephone line at the LDC. * Approximately 110 million words of text in French, German and Portuguese have been made available from Agence France Presse. Each language was supplied in separate data streams collected via a Dateno MKII satellite receiver and associated equipment at the LDC. * Approximately 20 million words of text in German have been made available from Deutsche Presse Agentur. The text is collected via an AP Datafeatures telephone line installed at the Linguistic Data Consortium. * A smaller part of the corpus comes from Le Monde newspaper. The Le Monde data covers about 5.6 million words of French. It is quite distinct from the AP and AFP materials in its markup approach, because it has been prepared in compliance with the conventions of the Text Encoding Initiative (TEI), rather than having been based on the model of the TIPSTER collections, which were originally developed prior to the establishment of the TEI conventions.  aContent in Portuguese, French, and German. Documentation in English. 0aPortuguese languagexData processingvDatabases. 0aFrench languagexData processingvDatabases. 0aGerman languagexData processingvDatabases.7 aExcerpts2lcgft10aGraff, David403University of Alberta Access (Request Form)uhttps://docs.google.com/forms/d/e/1FAIpQLSd4VsEYOWoubQww-01W7IV2qDaAr4ctBJUhrJvfyN0GwoMuFQ/viewform423Dataset documentationuhttps://catalog.ldc.upenn.edu/LDC95T1103502nmm a22003853i 4500006001900000007001500019008004100034020001500075024001500090024002900105042000700134245002800141264007300169336003800242336003300280337002600313338003200339500001500371500004500386506005100431520205900482520005102541546004202592650005002634650006602684655002802750700001902778700001802797700002202815700002002837700002102857700002102878856014902899856006803048m     o  u        cu ||||||u||||      s2015    pau        u        eng d  a15856373518 aLDC2015S128 aISLRN: 607-221-014-735-8  adc00aArticulation Index LSCP  a[Philadelphia, Pennsylvania]: bLinguistic Data Consortium, c[2015]  acomputer datasetbcod2rdacontent  aspoken wordbspw2rdacontent  acomputerbc2rdamedia  aunspecifiedbzu2rdacarrier  aLDC2015S12  ahttps://catalog.ldc.upenn.edu/LDC2015S121 aAvailable to University of Alberta users only.  aArticulation Index LSCP was developed by researchers at Laboratoire de Sciences Cognitives et Psycholinguistique (LSCP), Ecole Normale Supérieure. It revises and enhances a subset of Articulation Index (AIC) (LDC2005S22), a corpus of persons speaking English syllables. Changes include the addition of forced alignment to sound files, time alignment of syllable utterances and format conversions. AIC consists of 20 American English speakers (12 males, 8 females) pronouncing syllables, some of which form actual words, but most of which are nonsense syllables. All possible Consonant-Vowel (CV) and Vowel-Consonant (VC) combinations were recorded for each speaker twice, once in isolation and once within a carrier-sentence, for a total of 25768 recorded syllables. *Data* Articulation Index LSCP alters AIC in the following ways. * Time-alignments for the onset and offset of each word and syllable were generated through forced-alignment with a standard HMM-GMM (Hidden Markov Model-Gaussian Mixture Model) ASR system. * The time-alignments for the beginning and end of the syllables (whether in isolation or within a carrier sentence) were manually adjusted. The time-alignments for the other words in carrier sentences were not manually adjusted. * The recordings of isolated syllables were cut according to the manual time-alignments to remove the silent portions at the beginning and end, and the time-alignments were altered to correspond to the cut recordings. * The file naming scheme was slightly altered for compatibility with the Kaldi speech recognition toolkit. * AIC contains a wide-band (16 KHz, 16-bit PCM) and a narrow-band (8 KHz, 8 bit u-law) version of the recordings distributed in sphere format. The LSCP version contains the wide-band version only distributed as wave files. This release does not include certain AIC triphone recordings (CVC, CCV or VCC). Audio data is presented as 16kHz 16-bit flac compressed .wav files. The flac compression was added for distribution, and documentation may refer to the files as .wav files.  aData samples are available on the LDC website.  aContent and documentation in English. 0aEnglish languagexData processingvDatabases. 0aEnglish languagexSpoken EnglishxData processingvDatabases.7 aSound recordings2lcgft10aSchatz, Thomas10aCao, Xuan-Nga10aKolesnikova, Anna10aBergvelt, Tomas10aWright, Jonathan10aDupoux, Emmanuel403University of Alberta Access (Request Form)uhttps://docs.google.com/forms/d/e/1FAIpQLSd4VsEYOWoubQww-01W7IV2qDaAr4ctBJUhrJvfyN0GwoMuFQ/viewform423Dataset documentationuhttps://catalog.ldc.upenn.edu/LDC2015S1204083nmm a22004813i 4500006001900000007001500019008004100034020001500075024001500090024002900105041001800134041002800152042000700180245002600187264007300213336003800286336002600324337002600350338003200376500001500408500004500423506005100468520238000519520005102899546007102950650005003021650005103071655002003122700002203142700002003164700001903184700002203203700001903225700001703244700003103261700001603292700002303308700001703331700001903348700001703367856014903384856006803533m     o  u        cu ||||||u||||      s2007    pau        u        eng d  a15856344098 aLDC2007T218 aISLRN: 722-221-552-342-80 aengachibeng07aengacmnbeng2iso639-3  adc00aOntoNotes Release 1.0  a[Philadelphia, Pennsylvania]: bLinguistic Data Consortium, c[2007]  acomputer datasetbcod2rdacontent  atextbtxt2rdacontent  acomputerbc2rdamedia  aunspecifiedbzu2rdacarrier  aLDC2007T21  ahttps://catalog.ldc.upenn.edu/LDC2007T211 aAvailable to University of Alberta users only.  aNatural language applications like machine translation, question answering, and summarization currently are forced to depend on impoverished text models like bags of words or n-grams, while the decisions that they are making ought to be based on the meanings of those words in context. That lack of semantics causes problems throughout the applications. Misinterpreting the meaning of an ambiguous word results in failing to extract data, incorrect alignments for translation, and ambiguous language models. Incorrect coreference resolution results in missed information (because a connection is not made) or incorrectly conflated information (due to false connections). Some richer semantic representation is badly needed. The OntoNotes project is a collaborative effort between BBN Technologies, the University of Colorado, the University of Pennsylvania, and the University of Southern California's Information Sciences Institute to produce such a resource. It aims to annotate a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, use net, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference). OntoNotes builds on two time-tested resources, following the Penn Treebank for syntax and the Penn PropBank for predicate-argument structure. Its semantic representation will include word sense disambiguation for nouns and verbs, with each word sense connected to an ontology, and coreference. The current goals call for annotation of over a million words each of English and Chinese, and half a million words of Arabic over five years. The authors wish to make this resource available to the natural language research community so that decoders for these phenomena can be trained to generate the same structure in new documents. Lessons learned over the years have shown that the quality of annotation is crucial if it is going to be used for training machine learning algorithms. Taking this cue, we ensure that each layer of annotation in OntoNotes will have at least 90% inter- annotator agreement. Our pilot studies have shown that predicate structure, word sense, ontology linking, and coreference can all be annotated rapidly and with better than 90% consistency.  aData samples are available on the LDC website.  aContent in English and Mandarin Chinese. Documentation in English. 0aEnglish languagexData processingvDatabases. 0aMandarin dialectsxData processingvDatabases.7 aExcerpts2lcgft10aWeischedel, Ralph10aPradhan, Sameer10aRamshaw, Lance10aMicciulla, Linnea10aPalmer, Martha10aXue, Nianwen10aMarcus, Mitchell P., 1950-10aTaylor, Ann10aBabko-Malaya, Olga10aHovy, Eduard10aBelvin, Robert10aHouston, Ann403University of Alberta Access (Request Form)uhttps://docs.google.com/forms/d/e/1FAIpQLSd4VsEYOWoubQww-01W7IV2qDaAr4ctBJUhrJvfyN0GwoMuFQ/viewform423Dataset documentationuhttps://catalog.ldc.upenn.edu/LDC2007T2108667nmm a22003733i 4500006001900000007001500019008004100034020001500075024001500090024002900105041002800134041004800162042000700210245004200217264007300259336003800332336002600370337002600396338003200422500001500454500004500469506005100514520707700565520005107642546013407693650004907827650006007876650005007936650005007986655002008036700002008056856014908076856006808225m     o  u        cu ||||||u||||      s2008    pau        u        yor d  a15856350068 aLDC2008L038 aISLRN: 973-344-578-516-80 ayoracpeaengaspabeng07ayoratrfaluqagulaengaspabeng2iso639-3  adc00aGlobal Yoruba Lexical Database v. 1.0  a[Philadelphia, Pennsylvania]: bLinguistic Data Consortium, c[2008]  acomputer datasetbcod2rdacontent  atextbtxt2rdacontent  acomputerbc2rdamedia  aunspecifiedbzu2rdacarrier  aLDC2008L03  ahttps://catalog.ldc.upenn.edu/LDC2008L031 aAvailable to University of Alberta users only.  aThe Global Yoruba Lexical Database v. 1.0 is a set of related dictionaries providing definitions and translations for over 450,000 words from the Yoruba language and its variants: Standard Yoruba (over 368,000 words), Gullah (over 3,600 words), Lucumí (over 8,000 words) and Trinidadian (over 1,000 words). Yoruba is a Niger-Congo language (sub classification: Kwa > Yoruboid) spoken natively by nearly 20 million people, the vast majority of them in southwestern Nigeria. There are also approximately a half million Yoruba speakers in Benin, as well as speakers in Togo and Ghana and among the emigrant populations in the United States and the United Kingdom. In addition, roughly two million people in Nigeria speak Yoruba as a second language. The Yoruba language diaspora is wide, stretching from southwestern Nigeria and Benin westward to the Caribbean and islands along the southeastern United States coast. Yoruba and other African dialects arrived in the Americas and the Caribbean as a consequence of the Atlantic slave trade. Throughout the region, Yoruba dialects blended with each other and with languages like Spanish and French to form a variety of creoles such as Gullah in the United States and Nagô in Brazil. Many of those creoles have become the language of liturgy and music in Cuba, Brazil, Argentina, Trinidad, Jamaica and parts of the United States and Canada. The ultimate goal of this dictionary is to provide coverage for all Yoruba dialects across the globe. For that reason, it will continue to be a work in progress. The current standard orthography is tone-driven. Yoruba has three tones: a high tone, a middle tone and a low tone. Each syllable in a Yoruban word must have at least one tone and long vowels may have two tones. While there are no explicit rising or falling tones, combinations of the languages three basic tones may produce the same effect. Grammatically, Yoruba is a Subject-Verb-Object (SVO) language. Verbs have no infinitive forms, past or present tense and typically have only a single syllable. Discrete auxiliary words provide information on the verb tense. Nor do Yoruba nouns have plural or singular form their number derives from the context in which the word occurs. The Yoruba dialect continuum consists of over fifteen varieties, with considerable phonological and lexical differences among them and some grammatical ones as well. Peripheral areas of dialectal regions often have some similarities to adjoining dialects. Standard Yoruba is a koine used for education, writing, broadcasting, and contact between speakers of different dialects. It is also called Literary Yoruba, common Yoruba, or simply Yoruba without qualification. Though in large part based on the Ò?yò? and Ibadan dialects, it incorporates several features from other dialects and has a simplified vowel harmony system and some other features not found in other Yoruba dialects. *Data * This release encompasses the following languages and dialects: Languages Description Number of words Yoruba->English This dictionary of Standard Yoruba contains detailed lexicographic entries which include the part of speech, the English definition of the Yoruba headword, cross references, examples in English and the morphemic decomposition of the Yoruba headword. 142,389 English->Yoruba This dictionary maps the English headword back to Standard Yoruba and includes the part of speech, Yoruba definition, and morphemic decomposition of the Yoruba word. 226,585 Gullah->English and Yoruba Gullah is a creole spoken in the coastal Low Country of South Carolina and Georgia in the United States. Although the language is no longer spoken to a great extent, its words are still commonly used for personal names and nicknames. The dictionary translates from Gullah headwords to English and to Standard Yoruba. 3,636 Lucumí->Spanish, English and Yoruba Lucumí is the ritual language of the Santeria religion practiced in Cuba. The Lucumí dictionary translates from a Lucumí headword to Cuban Spanish to English to Standard Yoruba. At the time of this publication in 2008, some entries do not have complete translations and only map from Lucumí to Cuban Spanish. 8,075 Trinidadian->English and Yoruba Trinidadian is a creole which blends English, French, Spanish and African languages. The Trinidadian dictionary presents those words that have Yoruban roots and maps from the Trinidadian headword to English and Standard Yoruba. 1,187 The dictionaries in this publication are presented in two formats, Toolbox databases and XML. Short for The Field Linguists Toolbox, Toolbox is a lexicographical database system published by SIL. SIL makes Toolbox freely available for download. In order to use the Global Yoruba Lexical Database v. 1.0, Toolbox must first be installed on the users local computer. The orthography of the text in the databases conforms to that presented to students in the Nigerian school system. The basic Yoruba alphabet is: a b d e e? f g gb h i j k l m n o o? p r s s? t u w y The letter gb is a digraph, two letters that combine to form a single phoneme. In written Yoruba, gb functions as a single letter. In the Toolbox presentation, this has been taken into account and the software sorts the words accordingly in all functions. The XML presentation has been sorted according to the above alphabet but is a static, flat file. For that reason, developers creating applications from the XML files will need to take into account the digraph when writing searching and reporting functions. As Yoruba is a tonal language, the written language uses additional diacritic marks to denote tones. The orthography uses three tones: * Low: denoted with a grave symbol () as in à * Mid: plain letter without diacritics * High: denoted with an acute (´) symbol as in á Both the Toolbox and XML presentations encode the text in Unicode UTF-8 using normalized form C. Unicode normalized forms govern the order in which letters and characters are composed and processed by software systems. Normalized form C is the standard form used by most web systems and is a W3C standard for the web. The Toolbox presentation uses the Aria Unicode MS font for display. The Tahoma and Lucida Grande fonts will also display the Yoruba alphabet under UTF-8 encoding. Since XML only provides information about document structure, fonts are not specified in the XML versions of the dictionaries. Displaying non-Western letters:Windows users will need to install and configure their computers for Extended Language support. To do this, open the Windows Control Panel and click the Regional and Language Options icon. In the Regional and Language Options window that opens, select the Languages pane. Under the Supplemental Language Support section, check both check boxes and click okay. Windows will as for your install disc and will install the modules needed to properly display complex and non Western letters. If users do not have their Windows install disc, they should contact their local system administrator to install Extended Language Support.  aData samples are available on the LDC website.  aContent in Yoruba, Trinidadian Creole English, Lucumi, Sea Island Creole English, English, and Spanish. Documentation in English. 0aYoruba languagexData processingvDatabases. 0aSea Islands Creole dialectxData processingvDatabases. 0aEnglish languagexData processingvDatabases. 0aSpanish languagexData processingvDatabases.7 aExcerpts2lcgft10aAwoyale, Yiwola403University of Alberta Access (Request Form)uhttps://docs.google.com/forms/d/e/1FAIpQLSd4VsEYOWoubQww-01W7IV2qDaAr4ctBJUhrJvfyN0GwoMuFQ/viewform423Dataset documentationuhttps://catalog.ldc.upenn.edu/LDC2008L03